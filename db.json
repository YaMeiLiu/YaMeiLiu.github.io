{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/yilia/source/love.js","path":"love.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/main.0cf68a.css","path":"main.0cf68a.css","modified":1,"renderable":1},{"_id":"themes/yilia/source/main.0cf68a.js","path":"main.0cf68a.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/mobile.992cbe.js","path":"mobile.992cbe.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/slider.e37972.js","path":"slider.e37972.js","modified":1,"renderable":1},{"_id":"themes/yilia/source/assets/alipay.jpg","path":"assets/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/yilia/source/assets/qq.jpg","path":"assets/qq.jpg","modified":1,"renderable":1},{"_id":"themes/yilia/source/assets/touxiang.jpg","path":"assets/touxiang.jpg","modified":1,"renderable":1},{"_id":"themes/yilia/source/assets/wechat.jpg","path":"assets/wechat.jpg","modified":1,"renderable":1},{"_id":"themes/yilia/source/assets/weixin.jpg","path":"assets/weixin.jpg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","path":"fonts/iconfont.16acc2.ttf","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","path":"fonts/default-skin.b257fa.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","path":"fonts/iconfont.45d7ee.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","path":"fonts/iconfont.8c627f.woff","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","path":"fonts/iconfont.b322fa.eot","modified":1,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","path":"fonts/tooltip.4004ff.svg","modified":1,"renderable":1},{"_id":"themes/yilia/source/css/declare.scss","path":"css/declare.scss","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/default-skin.png","path":"img/default-skin.png","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/preloader.gif","path":"img/preloader.gif","modified":1,"renderable":1},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","path":"img/scrollbar_arrow.png","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0}],"Cache":[{"_id":"source/CNAME","hash":"9c6d86356d3f64ecc49f6add4c59b311534523ec","modified":1630769936925},{"_id":"source/categories/index.md","hash":"9e219e36ac196a5877092ec77a79fa716b92d663","modified":1630506045948},{"_id":"source/_posts/HLA-Face阅读笔记.md","hash":"ba766f9c47a744732c82f8b0e18eeab5915e59d3","modified":1630848332269},{"_id":"source/_posts/VS-code环境设置.md","hash":"63584af4467fa8ef1f2d3d1358fe443ae630f7ba","modified":1662529912708},{"_id":"source/_posts/目标检测中的NMS.md","hash":"4aca2b38b568152661857a88222b485bea744571","modified":1661759982121},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905162418404.png","hash":"f40c483e933fb8a81a26f29c263eb0a4e3807586","modified":1630830281395},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905192726753.png","hash":"29958270e0d3211daae70d58ac6a82d2d8d9153c","modified":1630841253812},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905192310760.png","hash":"68433f221f0d7826174d86422dd9f97a54b81b86","modified":1630840995788},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905194211096.png","hash":"739dcf1eb7024661f098649cde6b85079ecaa763","modified":1630842135172},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195532167.png","hash":"35bec287c8b2b2b026cbce7f94c0d1b5b652d8e2","modified":1630842936445},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195923509.png","hash":"a7300ebc27fff5d339b3b90ec0610f3e978d4dca","modified":1630843168004},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195112074.png","hash":"8270bcacba344917e5c1aa290aa30b5c3c912dbd","modified":1630842677231},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905202018226.png","hash":"f505399984dd8ba5c7badf5efa20e3ce12c4c33e","modified":1630844424223},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_4.jpg","hash":"f6697108d58a2b74c431de0a275a8872c2c6ac8e","modified":1628692966772},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_5.png","hash":"17f226699222941c914c18aa10e5382b090b0936","modified":1630505087253},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905203317458.png","hash":"22837227022fb0adde4a83f6dd3b9e7c623d217c","modified":1630845374221},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_2.png","hash":"0f6ccb9ab6997acd5343704e7c29378271552f9e","modified":1628691160513},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_3.jpg","hash":"b6b837fbb6e7046c6422e686f611bd803dee9afc","modified":1628692784045},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905153749362.png","hash":"4df1e9e7c70cad696f3767a47aec2daac6aa4308","modified":1630827469401},{"_id":"themes/yilia/layout/_partial/toc.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1510733361000},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905155032987.png","hash":"75720adfcf7c75f002bf4b42adf18a8756043225","modified":1630828256081},{"_id":"themes/yilia/.babelrc","hash":"b1b76475ac17dc9e2fa50af96c9e31eea2d0f2b4","modified":1510733361000},{"_id":"themes/yilia/.editorconfig","hash":"da6d022b8f4d9c961e2f8f80677e92af8de0db4d","modified":1510733361000},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905184019018.png","hash":"125f04b0b0804ca58f5f927a8c99e4ae1459c3ac","modified":1630838426312},{"_id":"themes/yilia/.gitignore","hash":"9c4b7d27a1e3e5efa0c8ed143a032a85d586b03b","modified":1510733361000},{"_id":"themes/yilia/.eslintignore","hash":"df0a50b13cc00acb749226fee3cee6e0351fb1d9","modified":1510733361000},{"_id":"themes/yilia/.gitattributes","hash":"e0f24dceeb1e6878a1dd9b01a2b9df1bc037a867","modified":1510733361000},{"_id":"themes/yilia/.eslintrc.js","hash":"5696ae049de010ed3786768b0c359f14c05b5ec6","modified":1510733361000},{"_id":"themes/yilia/README.md","hash":"1bf755806af9d8874bd22e1abbdaaa24328ef4dc","modified":1510733361000},{"_id":"themes/yilia/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1510733361000},{"_id":"themes/yilia/_config.yml","hash":"245e2cc43f0c1b11e111e20f5a5712c1a52b42c9","modified":1630768964910},{"_id":"themes/yilia/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1510733361000},{"_id":"themes/yilia/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1510733361000},{"_id":"themes/yilia/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1510733361000},{"_id":"themes/yilia/layout/categories.ejs","hash":"a88532de75d4a6805542a445a92ab2dbb162165f","modified":1630507199729},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1510733361000},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1510733361000},{"_id":"themes/yilia/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1510733361000},{"_id":"themes/yilia/package.json","hash":"367cb9579d35968a942c243ab248a5f5ebfaf462","modified":1510733361000},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1510733361000},{"_id":"themes/yilia/layout/index.ejs","hash":"a35dc900203f9d8dd863ea4c1722198d6d457ec8","modified":1510733361000},{"_id":"themes/yilia/webpack.config.js","hash":"05ba46a4ae744272f5312e684928910dccad3755","modified":1510733361000},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1510733361000},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1510733361000},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1510733361000},{"_id":"themes/yilia/layout/layout.ejs","hash":"0a332bdbd3b86c231d690614687f5b97186b85d5","modified":1628783755722},{"_id":"themes/yilia/layout/post.ejs","hash":"92dd7f45bb36fc64eb2ad592d9d45786b3582790","modified":1623507837871},{"_id":"themes/yilia/source/love.js","hash":"4345136020d12798c907c9094d0be50770f61f1a","modified":1623508416841},{"_id":"themes/yilia/source-src/css.ejs","hash":"94dbdb02ca11849e415d54fb28546a598f2cffb1","modified":1510733361000},{"_id":"themes/yilia/source/main.0cf68a.css","hash":"25ef57b66a472063ec8e1990985fe2361cb05b9a","modified":1630506979822},{"_id":"themes/yilia/source-src/script.ejs","hash":"c21381e1317db7bb157f1d182b8c088cb7cba411","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"1f7d4819b7f67602c4a1b99871808d2160b60978","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a6e94061ac55b9eb55275f87b608d62f6ea35659","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"d09ce079722619f0d2a96dd21d659a3a659e2d9e","modified":1628784171159},{"_id":"themes/yilia/layout/_partial/aside.ejs","hash":"8edbd7993b9b061611a193533a664e2e85eae748","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/baidu-analytics.ejs","hash":"f0e6e88f9f7eb08b8fe51449a8a3016273507924","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"b12eec32a00ab2417cdf816cbc9ef2ef2231a8c9","modified":1623600819548},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"7a12db840a7ef3eaf295ba99889a103c9952e098","modified":1623508497237},{"_id":"themes/yilia/layout/_partial/css.ejs","hash":"236f8a377b2e4e35754319c3029bcd4a4115431d","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"64f092186b5a744aa1603ce22bb1d44a34446add","modified":1623602040888},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"7666e777b98fb964bee2e0b16a52b7d8108ca401","modified":1630815319224},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"6387a93dad7c3d778eb91e3821852fbf6813880c","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"7fbbfabf5e29525b24ada14613c21a26789132b4","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"151a1ef2173ba7b6789d349f0f8da89616cc1394","modified":1510733361000},{"_id":"themes/yilia/source/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"e495790b2abe2290875817e42bd505bc611d3e26","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"c41341b9618e591538e1136a2d1637587c1bbd90","modified":1510733361000},{"_id":"themes/yilia/source/assets/touxiang.jpg","hash":"03ec37eba7c4b66306453dcc8170a1524287aa6e","modified":1623504383690},{"_id":"themes/yilia/source/assets/qq.jpg","hash":"6da8385e47181610fd3c1cedebca1b6851697d99","modified":1623504491901},{"_id":"themes/yilia/source/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1510733361000},{"_id":"themes/yilia/source/assets/weixin.jpg","hash":"ec9918a33e860998583414c3da34d6d5f86e5301","modified":1623504475664},{"_id":"themes/yilia/source/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1510733361000},{"_id":"themes/yilia/source/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1510733361000},{"_id":"themes/yilia/source/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1510733361000},{"_id":"themes/yilia/source/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1510733361000},{"_id":"themes/yilia/source/css/declare.scss","hash":"612a1a6bd5bb5e2e58d0833c75be84f8f49614ed","modified":1623596813607},{"_id":"themes/yilia/source/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1510733361000},{"_id":"themes/yilia/source/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1510733361000},{"_id":"themes/yilia/source/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1510733361000},{"_id":"themes/yilia/source/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1510733361000},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"24f347a2412abbf58318369152504da9538f8d3b","modified":1510733361000},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"93a50dd19a93485712da1f8d0a1672482dd1eabc","modified":1510733361000},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"7d27e22ac898e8fafec14549e940c73cbea1fba8","modified":1510733361000},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"d79f2d35a06de83a2a226ca790b7a0a34789c115","modified":1510733361000},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"3fad68bd74260326f83090b0974dd80707e7bac7","modified":1510733361000},{"_id":"themes/yilia/source-src/css/aside.scss","hash":"578a67464dd0f542197f7fcee158c991db058563","modified":1510733361000},{"_id":"themes/yilia/source-src/css/article.scss","hash":"0f6d61af99ed4db87f8589db1feaea7747b55963","modified":1510733361000},{"_id":"themes/yilia/source-src/css/comment.scss","hash":"cafe3834017a3bf47420f37543725025225a2c89","modified":1510733361000},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7c995410b25baaf61dfc5e148e22ca60330abcd3","modified":1510733361000},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"97b8fba41c914145710b90091f400b845879577f","modified":1510733361000},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"43e507f2a48504079afd9471353337e23ca47470","modified":1510733361000},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1510733361000},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"849a29fcd7150214fcf7b9715fa5dc71d1f9b896","modified":1510733361000},{"_id":"themes/yilia/source-src/css/left.scss","hash":"0d30c0e7cdb831c3881a017006c782f2214ac195","modified":1510733361000},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"f053c609d84df0dd9eee1d11ddf0c19163a456be","modified":1510733361000},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"3719994c2c9393813cc1d42b657205c368a329cb","modified":1510733361000},{"_id":"themes/yilia/source-src/css/main.scss","hash":"3d25e698a56ac964918bc52b40aa7cd505abf0f9","modified":1623596482115},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"80a4fcf9171d4a33235da96ac8a2b7dcabc45dfb","modified":1510733361000},{"_id":"themes/yilia/source-src/css/page.scss","hash":"bf206bb7f7d0967bc8b7fdf01b7ffc99aff9ba88","modified":1510733361000},{"_id":"themes/yilia/source-src/css/share.scss","hash":"150c6425f6582e7ec78a873256ce49c9930e8805","modified":1510733361000},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"ace041d72f95b419f6a5e443191703c2b62007f4","modified":1510733361000},{"_id":"themes/yilia/source-src/css/social.scss","hash":"724162ccf3977e70a45d189abfaa20b6e2fba87b","modified":1510733361000},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"9c8dfd1c76854ef063494ca76fac6360b391ed6d","modified":1510733361000},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"1b1aa0908e58cf942b28e3881d07c5573c4129e1","modified":1510733361000},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"c8aa84fca93862d3caae77c552873b8610f33327","modified":1510733361000},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"ac67a3c7097849206244db9b0ba91daaba017ef5","modified":1510733361000},{"_id":"themes/yilia/source-src/js/anm.js","hash":"4a4c5d82b09a3063f91a434388e6aa064fd7fd98","modified":1510733361000},{"_id":"themes/yilia/source-src/js/aside.js","hash":"754f771264548a6c5a8ad842908e59ae4e7ed099","modified":1510733361000},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"53d5a554bc2f38e9bb3d26400a47767013c05216","modified":1510733361000},{"_id":"themes/yilia/source-src/js/browser.js","hash":"04095b38cfd4316a23f8eb14b1ffaf95f78a4260","modified":1510733361000},{"_id":"themes/yilia/source-src/js/main.js","hash":"3894e60827c817319e43c9ff3ed045fc3d7336ce","modified":1510733361000},{"_id":"themes/yilia/source-src/js/fix.js","hash":"d6782d53c992e712af39c84e804eccaf38830b94","modified":1510733361000},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"4d823b039fd296d24a454eae5a798b93c44560cb","modified":1510733361000},{"_id":"themes/yilia/source-src/js/report.js","hash":"4f1d9a18a936ce40b037f636a39127dd19175b6e","modified":1510733361000},{"_id":"themes/yilia/source-src/js/share.js","hash":"b090f82cf80cba7da764753906d9e2cc2acdf30d","modified":1510733361000},{"_id":"themes/yilia/source-src/js/slider.js","hash":"e846bcc5aac9c68b93f7b8de353df54d8d29f666","modified":1510733361000},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"2577deb6a9fe4f5436360b2ce9afcc7f9a7f0116","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/changyan.ejs","hash":"5f99b55980da64a723a8e14d5a7daba0d6504647","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"0809a4829aabeb4e911a3ed04ec28db4df7dfe3f","modified":1510733361000},{"_id":"themes/yilia/source-src/js/Q.js","hash":"d011af172064b6c6e0c7051d8f9879373ddac113","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"ef71c4081e866a494367575c59610e7e6339ece0","modified":1510733361000},{"_id":"themes/yilia/source-src/js/util.js","hash":"8456e9d6b19532742582c99b2fb9d09e146e1c58","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/gitment.ejs","hash":"e68bbac9ffb1ad27b56837c9abad6ed6bb7daa0c","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"1036c8e4e1a7bc935ba173744da735a0d6ed09cd","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"e8399025ed3b980aedb821c92855889f5f12fd5b","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"2e783e68755abb852760eb0e627a3fbb50a05a55","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/wangyiyun.ejs","hash":"ea41c462168d9697caef9485862e9cac718a12c1","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/leancloud.ejs","hash":"e16cd43f34b26e1cd41979bb27669fd2a931e65a","modified":1623591509881},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"8935d9c1e4a4b14592f83ee68c65720460c701ac","modified":1623591263235},{"_id":"themes/yilia/layout/_partial/post/valine.ejs","hash":"cfbe5beb3929e560146ade8749b741ea72277914","modified":1623511250778},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"63a37f26276f9207405afe0f2d65339ce295bbaf","modified":1510733361000},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"491ab3378d5c11005ba65c607608bb36b368a9d5","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/post/word.ejs","hash":"3206ef8b47dd2e402342daeae01b5099bbc36ca9","modified":1623602853275},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"88ef2dc732e8887be56709e0d465a1674bd5fb2f","modified":1623507589875},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"fb511c505d1309249f21dc577d4ad2bad99a764f","modified":1510733361000},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"3bba5c77bad5981eac859fe05c9561d580ba7fa9","modified":1510733361000},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1510733361000},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"fab871fa93bd542e76a71a56428f2994a4aaf443","modified":1510733361000},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1510733361000},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1510733361000},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1510733361000},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1510733361000},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1510733361000},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1510733361000},{"_id":"themes/yilia/source/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1510733361000},{"_id":"themes/yilia/layout/_partial/script.ejs","hash":"4cb685f07e89dd5175c2a576e73a1a957aec5637","modified":1510733361000},{"_id":"themes/yilia/source/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1510733361000},{"_id":"themes/yilia/source/assets/wechat.jpg","hash":"18c1b6c3482449ceaa0f749eb902ed20095336ec","modified":1623504974610},{"_id":"themes/yilia/source/assets/alipay.jpg","hash":"166998eeb397bc3d03a26fc84f85397c3eb046fe","modified":1623505010537},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_1.png","hash":"322663833ba03f09298c7761b00d164279cca6ef","modified":1628690485059},{"_id":"public/content.json","hash":"7ab7a34bfdcb806f56d03ee450ed870c0ea629a3","modified":1662562402546},{"_id":"public/categories/index.html","hash":"71a46fa425f9c4424ef729bb51a5fe6d283638d0","modified":1662562402546},{"_id":"public/2022/08/29/VS-code环境设置/index.html","hash":"c8a33f716cad8c9a2b6b3629b25f9138c7186c78","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/index.html","hash":"5a5dc3964c5426783dd68a55e0e3a61800dd556b","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/index.html","hash":"796f48239a3f9d3712c6de175a10ed8868ca9bc2","modified":1662562402546},{"_id":"public/archives/index.html","hash":"2c76cd17969c7055c9f87e7916f2e8223fe9569b","modified":1662562402546},{"_id":"public/archives/2021/index.html","hash":"37f39c93b5292156ec9f9550c102a502bd897310","modified":1662562402546},{"_id":"public/archives/2021/06/index.html","hash":"98306417e82b57790062569157ff869292068e03","modified":1662562402546},{"_id":"public/archives/2021/09/index.html","hash":"7f1f63bee1222d54b1f0d9ea0558cd492f900702","modified":1662562402546},{"_id":"public/archives/2022/index.html","hash":"2390a46eec68f385b0b1ecb5cc9d02b2334b1719","modified":1662562402546},{"_id":"public/archives/2022/08/index.html","hash":"b5db683d263a9d1f0d751fe94635313b1e67ecd6","modified":1662562402546},{"_id":"public/index.html","hash":"05d2895443238f4bf8b47706bfd483de747a8508","modified":1662562402546},{"_id":"public/categories/图像处理/index.html","hash":"adb57a23df913264a82d0c0d799432a8b61f460a","modified":1662562402546},{"_id":"public/categories/工具/index.html","hash":"1688ef307be20aca5dfd3d54ad0dcc8a4486e294","modified":1662562402546},{"_id":"public/categories/图像处理/目标检测/index.html","hash":"9f513fe05f76bb2788c5625ae61ef51ee8e09d2b","modified":1662562402546},{"_id":"public/tags/随笔/index.html","hash":"a53f2f7d38a9715eb86aca5d4a08872e39025a12","modified":1662562402546},{"_id":"public/assets/qq.jpg","hash":"6da8385e47181610fd3c1cedebca1b6851697d99","modified":1662562402546},{"_id":"public/assets/touxiang.jpg","hash":"03ec37eba7c4b66306453dcc8170a1524287aa6e","modified":1662562402546},{"_id":"public/assets/weixin.jpg","hash":"ec9918a33e860998583414c3da34d6d5f86e5301","modified":1662562402546},{"_id":"public/fonts/default-skin.b257fa.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1662562402546},{"_id":"public/fonts/iconfont.16acc2.ttf","hash":"f342ac8bf4d937f42a7d6a0032ad267ab47eb7f2","modified":1662562402546},{"_id":"public/fonts/iconfont.45d7ee.svg","hash":"f9304e5714d20861be7d8f4d36687e88e86b9e1b","modified":1662562402546},{"_id":"public/fonts/iconfont.8c627f.woff","hash":"aa9672cb097f7fd73ae5a03bcd3d9d726935bc0a","modified":1662562402546},{"_id":"public/fonts/iconfont.b322fa.eot","hash":"bc8c5e88f4994a852041b4d83f126d9c4d419b4a","modified":1662562402546},{"_id":"public/css/declare.scss","hash":"612a1a6bd5bb5e2e58d0833c75be84f8f49614ed","modified":1662562402546},{"_id":"public/img/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1662562402546},{"_id":"public/fonts/tooltip.4004ff.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1662562402546},{"_id":"public/img/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1662562402546},{"_id":"public/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1662562402546},{"_id":"public/CNAME","hash":"9c6d86356d3f64ecc49f6add4c59b311534523ec","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905162418404.png","hash":"f40c483e933fb8a81a26f29c263eb0a4e3807586","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905192310760.png","hash":"68433f221f0d7826174d86422dd9f97a54b81b86","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905192726753.png","hash":"29958270e0d3211daae70d58ac6a82d2d8d9153c","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905194211096.png","hash":"739dcf1eb7024661f098649cde6b85079ecaa763","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905195112074.png","hash":"8270bcacba344917e5c1aa290aa30b5c3c912dbd","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905195923509.png","hash":"a7300ebc27fff5d339b3b90ec0610f3e978d4dca","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905202018226.png","hash":"f505399984dd8ba5c7badf5efa20e3ce12c4c33e","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905195532167.png","hash":"35bec287c8b2b2b026cbce7f94c0d1b5b652d8e2","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/blog1_fig_4.jpg","hash":"f6697108d58a2b74c431de0a275a8872c2c6ac8e","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/blog1_fig_5.png","hash":"17f226699222941c914c18aa10e5382b090b0936","modified":1662562402546},{"_id":"public/assets/wechat.jpg","hash":"18c1b6c3482449ceaa0f749eb902ed20095336ec","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905203317458.png","hash":"22837227022fb0adde4a83f6dd3b9e7c623d217c","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/blog1_fig_2.png","hash":"0f6ccb9ab6997acd5343704e7c29378271552f9e","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/blog1_fig_3.jpg","hash":"b6b837fbb6e7046c6422e686f611bd803dee9afc","modified":1662562402546},{"_id":"public/assets/alipay.jpg","hash":"166998eeb397bc3d03a26fc84f85397c3eb046fe","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905153749362.png","hash":"4df1e9e7c70cad696f3767a47aec2daac6aa4308","modified":1662562402546},{"_id":"public/love.js","hash":"4345136020d12798c907c9094d0be50770f61f1a","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905155032987.png","hash":"75720adfcf7c75f002bf4b42adf18a8756043225","modified":1662562402546},{"_id":"public/2021/09/04/HLA-Face阅读笔记/image-20210905184019018.png","hash":"125f04b0b0804ca58f5f927a8c99e4ae1459c3ac","modified":1662562402546},{"_id":"public/main.0cf68a.css","hash":"8daccb8afdf13bb691d3e932b9702cf13ec96776","modified":1662562402546},{"_id":"public/slider.e37972.js","hash":"6dec4e220c89049037eebc44404abd8455d22ad7","modified":1662562402546},{"_id":"public/main.0cf68a.js","hash":"993fadeb5f6d296e9d997a49ee20dc97333ceab7","modified":1662562402546},{"_id":"public/2021/06/13/目标检测中的NMS/blog1_fig_1.png","hash":"322663833ba03f09298c7761b00d164279cca6ef","modified":1662562402546},{"_id":"public/mobile.992cbe.js","hash":"01b35e71e37aa2849664eb5daf26daede2278398","modified":1662562402546}],"Category":[{"name":"图像处理","_id":"cl7rqq9gw0003jsvaa0km1muh"},{"name":"工具","_id":"cl7rqq9h30006jsva8u5k0dhr"},{"name":"目标检测","parent":"cl7rqq9gw0003jsvaa0km1muh","_id":"cl7rqq9hb0009jsvadh8z3rg0"}],"Data":[],"Page":[{"title":"文章分类","date":"2021-09-01T14:18:18.000Z","type":"categories","layout":"categories","comments":0,"_content":"\n","source":"categories/index.md","raw":"---\ntitle: 文章分类\ndate: 2021-09-01 22:18:18\ntype: \"categories\"\nlayout: \"categories\"\ncomments: false #关闭评论\n---\n\n","updated":"2021-09-01T14:20:45.948Z","path":"categories/index.html","_id":"cl7rqq9g30000jsvagz6nb1xc","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"hla-face:","title":"HLA-Face 阅读笔记","date":"2021-09-04T11:38:01.000Z","_content":"\n [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984v1) 收录于CVPR2021，提升弱光环境人脸的检出率。\n\n[源代码](https://daooshee.github.io/HLA-Face-Website/  )\n\n### 摘要\n\n弱光环境下的人脸检测在现实中有着非常重要的应用，如夜间监控视频和自动驾驶等。目前多数人脸检测强依赖于大量的标注，而制作标准数据是非常费时费力的，为了减少制作弱光条件下建立新数据的负担，作者利用现有的正常光下的数据，探索如何适应正常光到弱光下的人脸检测。该任务的难点在于：正常光场景和弱光场景存在巨大且复杂的差异，现有的弱光增强和自适应方式不能达到很好的效果，因为，作者提出了一种联合高低适应（High-Low Adaptation ，HLA) ）框架，具体地，通过一个双向的low-level适应和多任务high-level适应，HLA在没有弱光标签进行训练的情况下，达到了最优效果。<!--more-->\n\n### 引言\n\n人脸检测是许多计算机视觉任务的基础，已广泛应用到实际生活中，如智能监控、人脸解锁以及手机美颜。在过去几十年里，人脸检测研究取得了很大的进展，然而对于不良光照下的场景，很好的检出人脸依旧很困难。在弱光条件下拍摄的图片会出现一系列的退化：能见度低、强烈的噪声和色彩失真。这些退化不仅会影响人类的视觉能力，还会使机器视觉任务性能恶化。\n\n弱光数据DARK FACE的提出催生了大量了弱光人脸检测研究。然而，目前的研究依赖于大量的标注，因此有较差的鲁棒性。\n\n本文作者提出在不需要DARK FACE标注的情况的下，将正常光下的人脸检测模型适应于弱光场景中。作者发现正常光场景和弱光场景存在两个层次的差异：(1)像素级别：不足光照强度、相机噪音、颜色偏差。（2）对象语义级别：街灯、汽车灯、广告牌等。传统的弱光增强方法是为了提高视觉质量而设计的，无法填补语义上的差异。为了人脸检测模型从正常光场景适应到弱光场景下，作者考虑了low-level和high-level的联合适应，提出了HLA框架。具体来说，对于low-level适应，典型方法要么使图像变亮，要么使图像变暗，然而这种单向的从low-to-normal 和从normal-to-low的转换不能达到理想的效果，因此提出让两个域相互靠近一步，通过提高弱光图像的亮度和扭曲正常光图像，建立了介于正常和弱光之间的中间状态。对于high-level自适应，使用多任务自监督学习来缩小由low-level自适应构建的中间状态的特征距离。通过high-level和low-level适应，即时不使用弱光数据标签进行训练，本文方法也比当前最好的人脸检测模型表现更好。总得贡献如下：\n\n- 通过联合high-level和low-level自适应，提出一种不使用弱光标签的弱光人脸检测框架，实现了当前最好性能。\n\n- 对于low-level自适应，设计了一个双向适应思路，对弱光数据进行增强，对正常光数据加入噪声和颜色扭曲色彩，建立中间状态，使两个域相互靠近。\n\n- 对于high-level自适应，引入跨域自监督学习进行特征自适应，通过基于上下文和对比学习，逼近多个域之间的特征距离，增强特征表达能力。\n\n### 弱光检测联合自适应\n\n#### 动机\n\n该任务是将在正常光数据（H）上训练的人脸检测网络适应到未被标注的弱光数据（L）上。如下图所示，现有方法大致分为enhancement, darkening, and feature adaptation  三类。基于enhancement的方式首先增亮弱光图像，然后直接对其进行测试，这类方法通常不需要微调模型，因此非常的灵活。基于darkening的方式首先将正常光的数据变暗，然后重新训练模型。基于特征自适应的方式大多使用对齐、对抗学习或伪标记直接自适应模型特征。\n\n### <img src=\"./HLA-Face阅读笔记/image-20210905153749362.png\" alt=\"image-20210905153749362\" style=\"zoom:80%;\" />\n\n暗光人脸检测的主要问题是H和L之间存在复杂且巨大的差异，目前方法很难处理。在WIDER FACE和DARK FACE中的数据不仅有不同的像素级差异：（明亮v黑暗，干净vs噪声），还包含不同的物体和场景（绘画vs街景）。然而enhancement, darkening只考虑了像素级的差异，Feature adaptation  方法尝试一步填补整个差异，但是效果有限。\n\n对此，作者提出HLA方案，如上图（e）所示，作者设置了L和H之间的中间状态，并根据这些状态自适应相应的高级表达。具体地说，通过增强和暗化来减少H和L之间的low-level距离，相比单向的L->H或H->L，作者提出的双向转换L->E(L)和H-> d (H)，更能缓解自适应的难度。然后通过将多个状态的特征空间相互推近来减少high-level之间的距离。在测试时，仅对图像进行L->E(L)的操作，然后用适应后的人脸检测器对E(L)进行测试。\n\n#### 双向low-level适应\n\n弱光降解是一个复杂的过程，作者将相关因素分为三个：光照、噪声和颜色失真。虽然去燥是颜色校正是非常困难的，但是添加噪声和扭曲颜色是比较容易的。在此基础上，将L进行亮化变成E(L)，对H加入噪声和扭曲颜色使其变成D(H)，与L和H相比，E(L)和D(H)更加相似。这样，我们就可以让H和L彼此走进一步，从而减轻自适应的困难。\n\n**Brightening**  ：与普通的弱光增强任务不同，这里作者想要在不去噪和颜色校正的情况下调整，此外，弱光图像受到不均匀照明的影响，有些人脸被路灯照亮，而有些人脸被黑暗笼罩。因此，还需要防止过度曝光和曝光不足。基于非线性曲线映射[15]，由迭代二次曲线LE(·)构成。\n\n<img src=\"HLA-Face阅读笔记/image-20210905162418404.png\" alt=\"image-20210905162418404\" style=\"zoom: 50%;\" />\n\nLE<sub>o</sub> 是输入图片，LE<sub>n</sub> 是迭代n次的结果。A<sub>n</sub> 是由神经网络估计的像素级别的三通道映射图。与常见的端到端或基于Retinex理论的深度增强方式，曲线映射[15]不会引入额外的噪声或伪影。直接按照[15]的方式进行弱光增强力度不够，很多人脸仍被黑暗笼罩，这是因为过多的增加进行增强会引入噪声。而在本文中，作者为了更多的提高图像亮度，将公式（1）中的迭代次数加倍和扩大曲线估计网络。虽然引入了噪声和使眼色失真，但是可以通过H->D(H)处理。\n\n![image-20210905184019018](HLA-Face阅读笔记/image-20210905184019018.png)\n\n**Noise Synthesis**：虽然可以通过Brightening减少像素之间的差距，但是E(L)和H之间的差距仍然具有挑战性。因此，作者将剩下的差距进一步分解为颜色和噪声。同时，通过分离颜色，可以利用颜色去引导噪声合成。\n\n如上图所示，首先利用一个双边滤波器对E(L)进行模糊化，然后利用Pix2Pix网络训练E(L)<sub>blur</sub> ->E(L)，这样就得到了一个添加噪声的网络。最后利用训练好的Pix2Pix网络对H进行添加噪声。H<sub>noise</sub> 成功的模拟了E(L)的噪声。\n\n**Color Jittering**：作者希望D(H)和E(L)之间有着相似的颜色分布，基于静态统计，作者设置了亮度、对比度、饱和度、色度的抖动范围分别在（0.4，1.2）、（0.6，1.4）、（0.6，1.4）和（0.8，1.2）。\n\n#### 多任务high-level适应\n\n特征自适应方法大多是基于对齐、伪标记和对抗学习，然而，对齐和伪标记不能很好的处理巨大的差距，而对抗学习不稳定。作者提出充分利用图像本身的自然信息，通过跨域的自监督分类学习，特征被迫映射到相同的高维子空间，从而缩小high-level之间的特征差距。\n\n为了使E(L)、H和D(H)之间相互靠近，首先通过基于上下文的跨域自监督学习使E(L)和H之间靠近，然后通过跨域对比学习拉近H和D(H)，还通过单域对比学习进一步增强E(L)的表示。以多任务的方式实现整个自适应工作。\n\n**Closing E(L) and H**：基于上下文自监督设计代理任务（pretext tasks），可以使模型学习和理解目标的空间背景信息。在这里，作者使用拼图游戏，作者指出旋转和结合旋转拼图，效果不如单独使用拼图。可能原因，在WIDER FACE数据中，很多图像都是绘画和广告，其中的脸可能有很奇怪的角度，因此旋转预测代理任务可能是模糊不清的。作者将3x3的图像块组成整幅图像，设置图像排列为30，P<sub>jip</sub>  作为标签排列，L<sub>c</sub>表示交叉熵： \n\n<img src=\"HLA-Face阅读笔记/image-20210905192310760.png\" alt=\"image-20210905192310760\" style=\"zoom:50%;\" />\n\nF<sub>jip</sub>表示相应的域提取的特征图，E(L)和H共享分类头，可以迫使语义特征映射到相同的空间中，从而缩小high-level之间的差距。最后拉近E(L)和H的损失为：\n\n<img src=\"HLA-Face阅读笔记/image-20210905192726753.png\" alt=\"image-20210905192726753\" style=\"zoom:50%;\" />\n\n**Closing H and D(H)**：对比学习的思想是给定一个 v，识别和它相似的样本v<sup>+</sup>，和它不相似的样本v<sup>-</sup>。利用点积来衡量相似性。目标损失为L<sub>q</sub>(v,v<sup>-</sup>,v<sup>+</sup>)。τ是个温度超参，直观的说，这是一个N+1分类的问题。为了减少H和D(H)之间的距离，作者利用对比学习使正样本更接近。这种方式可以提高H和D(H)之间的特性相似性，拉近high-level之间的差距。\n\n<img src=\"HLA-Face阅读笔记/image-20210905194211096.png\" alt=\"image-20210905194211096\" style=\"zoom:50%;\" />\n\n<img src=\"HLA-Face阅读笔记/image-20210905195112074.png\" alt=\"image-20210905195112074\" style=\"zoom:50%;\" />\n\n在H和D(H)引入单域对比学习，D(•)表示增广的一部分。简化后损失为公式9，D<sup>*</sup>(H)有50%的概率为H，有50%的概率为D(H)：\n\n<img src=\"HLA-Face阅读笔记/image-20210905195532167.png\" alt=\"image-20210905195532167\" style=\"zoom:50%;\" />\n\n**Enhancing E(L)**：增强E(L)同样也使用对比学习。\n\n<img src=\"HLA-Face阅读笔记/image-20210905195923509.png\" alt=\"image-20210905195923509\" style=\"zoom:50%;\" />\n\n**Final objective**：最终的损失函数。\n\n<img src=\"HLA-Face阅读笔记/image-20210905202018226.png\" alt=\"image-20210905202018226\" style=\"zoom:50%;\" />\n\n### 实验结果\n\n和当前的一些人脸检测模型、以及基于enhancement, darkening, and feature adaptation 三类方式的适应方式进行对比。\n\n![image-20210905203317458](HLA-Face阅读笔记/image-20210905203317458.png)\n\n\n\n参文：\n\n[15]Zero-referencedeep curve estimation for low-light image enhancement  \n","source":"_posts/HLA-Face阅读笔记.md","raw":"---\nlayout: 'hla-face:'\ntitle: HLA-Face 阅读笔记\ndate: 2021-09-04 19:38:01\ntags:\ncategories:\n- 图像处理\n- 目标检测\n---\n\n [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984v1) 收录于CVPR2021，提升弱光环境人脸的检出率。\n\n[源代码](https://daooshee.github.io/HLA-Face-Website/  )\n\n### 摘要\n\n弱光环境下的人脸检测在现实中有着非常重要的应用，如夜间监控视频和自动驾驶等。目前多数人脸检测强依赖于大量的标注，而制作标准数据是非常费时费力的，为了减少制作弱光条件下建立新数据的负担，作者利用现有的正常光下的数据，探索如何适应正常光到弱光下的人脸检测。该任务的难点在于：正常光场景和弱光场景存在巨大且复杂的差异，现有的弱光增强和自适应方式不能达到很好的效果，因为，作者提出了一种联合高低适应（High-Low Adaptation ，HLA) ）框架，具体地，通过一个双向的low-level适应和多任务high-level适应，HLA在没有弱光标签进行训练的情况下，达到了最优效果。<!--more-->\n\n### 引言\n\n人脸检测是许多计算机视觉任务的基础，已广泛应用到实际生活中，如智能监控、人脸解锁以及手机美颜。在过去几十年里，人脸检测研究取得了很大的进展，然而对于不良光照下的场景，很好的检出人脸依旧很困难。在弱光条件下拍摄的图片会出现一系列的退化：能见度低、强烈的噪声和色彩失真。这些退化不仅会影响人类的视觉能力，还会使机器视觉任务性能恶化。\n\n弱光数据DARK FACE的提出催生了大量了弱光人脸检测研究。然而，目前的研究依赖于大量的标注，因此有较差的鲁棒性。\n\n本文作者提出在不需要DARK FACE标注的情况的下，将正常光下的人脸检测模型适应于弱光场景中。作者发现正常光场景和弱光场景存在两个层次的差异：(1)像素级别：不足光照强度、相机噪音、颜色偏差。（2）对象语义级别：街灯、汽车灯、广告牌等。传统的弱光增强方法是为了提高视觉质量而设计的，无法填补语义上的差异。为了人脸检测模型从正常光场景适应到弱光场景下，作者考虑了low-level和high-level的联合适应，提出了HLA框架。具体来说，对于low-level适应，典型方法要么使图像变亮，要么使图像变暗，然而这种单向的从low-to-normal 和从normal-to-low的转换不能达到理想的效果，因此提出让两个域相互靠近一步，通过提高弱光图像的亮度和扭曲正常光图像，建立了介于正常和弱光之间的中间状态。对于high-level自适应，使用多任务自监督学习来缩小由low-level自适应构建的中间状态的特征距离。通过high-level和low-level适应，即时不使用弱光数据标签进行训练，本文方法也比当前最好的人脸检测模型表现更好。总得贡献如下：\n\n- 通过联合high-level和low-level自适应，提出一种不使用弱光标签的弱光人脸检测框架，实现了当前最好性能。\n\n- 对于low-level自适应，设计了一个双向适应思路，对弱光数据进行增强，对正常光数据加入噪声和颜色扭曲色彩，建立中间状态，使两个域相互靠近。\n\n- 对于high-level自适应，引入跨域自监督学习进行特征自适应，通过基于上下文和对比学习，逼近多个域之间的特征距离，增强特征表达能力。\n\n### 弱光检测联合自适应\n\n#### 动机\n\n该任务是将在正常光数据（H）上训练的人脸检测网络适应到未被标注的弱光数据（L）上。如下图所示，现有方法大致分为enhancement, darkening, and feature adaptation  三类。基于enhancement的方式首先增亮弱光图像，然后直接对其进行测试，这类方法通常不需要微调模型，因此非常的灵活。基于darkening的方式首先将正常光的数据变暗，然后重新训练模型。基于特征自适应的方式大多使用对齐、对抗学习或伪标记直接自适应模型特征。\n\n### <img src=\"./HLA-Face阅读笔记/image-20210905153749362.png\" alt=\"image-20210905153749362\" style=\"zoom:80%;\" />\n\n暗光人脸检测的主要问题是H和L之间存在复杂且巨大的差异，目前方法很难处理。在WIDER FACE和DARK FACE中的数据不仅有不同的像素级差异：（明亮v黑暗，干净vs噪声），还包含不同的物体和场景（绘画vs街景）。然而enhancement, darkening只考虑了像素级的差异，Feature adaptation  方法尝试一步填补整个差异，但是效果有限。\n\n对此，作者提出HLA方案，如上图（e）所示，作者设置了L和H之间的中间状态，并根据这些状态自适应相应的高级表达。具体地说，通过增强和暗化来减少H和L之间的low-level距离，相比单向的L->H或H->L，作者提出的双向转换L->E(L)和H-> d (H)，更能缓解自适应的难度。然后通过将多个状态的特征空间相互推近来减少high-level之间的距离。在测试时，仅对图像进行L->E(L)的操作，然后用适应后的人脸检测器对E(L)进行测试。\n\n#### 双向low-level适应\n\n弱光降解是一个复杂的过程，作者将相关因素分为三个：光照、噪声和颜色失真。虽然去燥是颜色校正是非常困难的，但是添加噪声和扭曲颜色是比较容易的。在此基础上，将L进行亮化变成E(L)，对H加入噪声和扭曲颜色使其变成D(H)，与L和H相比，E(L)和D(H)更加相似。这样，我们就可以让H和L彼此走进一步，从而减轻自适应的困难。\n\n**Brightening**  ：与普通的弱光增强任务不同，这里作者想要在不去噪和颜色校正的情况下调整，此外，弱光图像受到不均匀照明的影响，有些人脸被路灯照亮，而有些人脸被黑暗笼罩。因此，还需要防止过度曝光和曝光不足。基于非线性曲线映射[15]，由迭代二次曲线LE(·)构成。\n\n<img src=\"HLA-Face阅读笔记/image-20210905162418404.png\" alt=\"image-20210905162418404\" style=\"zoom: 50%;\" />\n\nLE<sub>o</sub> 是输入图片，LE<sub>n</sub> 是迭代n次的结果。A<sub>n</sub> 是由神经网络估计的像素级别的三通道映射图。与常见的端到端或基于Retinex理论的深度增强方式，曲线映射[15]不会引入额外的噪声或伪影。直接按照[15]的方式进行弱光增强力度不够，很多人脸仍被黑暗笼罩，这是因为过多的增加进行增强会引入噪声。而在本文中，作者为了更多的提高图像亮度，将公式（1）中的迭代次数加倍和扩大曲线估计网络。虽然引入了噪声和使眼色失真，但是可以通过H->D(H)处理。\n\n![image-20210905184019018](HLA-Face阅读笔记/image-20210905184019018.png)\n\n**Noise Synthesis**：虽然可以通过Brightening减少像素之间的差距，但是E(L)和H之间的差距仍然具有挑战性。因此，作者将剩下的差距进一步分解为颜色和噪声。同时，通过分离颜色，可以利用颜色去引导噪声合成。\n\n如上图所示，首先利用一个双边滤波器对E(L)进行模糊化，然后利用Pix2Pix网络训练E(L)<sub>blur</sub> ->E(L)，这样就得到了一个添加噪声的网络。最后利用训练好的Pix2Pix网络对H进行添加噪声。H<sub>noise</sub> 成功的模拟了E(L)的噪声。\n\n**Color Jittering**：作者希望D(H)和E(L)之间有着相似的颜色分布，基于静态统计，作者设置了亮度、对比度、饱和度、色度的抖动范围分别在（0.4，1.2）、（0.6，1.4）、（0.6，1.4）和（0.8，1.2）。\n\n#### 多任务high-level适应\n\n特征自适应方法大多是基于对齐、伪标记和对抗学习，然而，对齐和伪标记不能很好的处理巨大的差距，而对抗学习不稳定。作者提出充分利用图像本身的自然信息，通过跨域的自监督分类学习，特征被迫映射到相同的高维子空间，从而缩小high-level之间的特征差距。\n\n为了使E(L)、H和D(H)之间相互靠近，首先通过基于上下文的跨域自监督学习使E(L)和H之间靠近，然后通过跨域对比学习拉近H和D(H)，还通过单域对比学习进一步增强E(L)的表示。以多任务的方式实现整个自适应工作。\n\n**Closing E(L) and H**：基于上下文自监督设计代理任务（pretext tasks），可以使模型学习和理解目标的空间背景信息。在这里，作者使用拼图游戏，作者指出旋转和结合旋转拼图，效果不如单独使用拼图。可能原因，在WIDER FACE数据中，很多图像都是绘画和广告，其中的脸可能有很奇怪的角度，因此旋转预测代理任务可能是模糊不清的。作者将3x3的图像块组成整幅图像，设置图像排列为30，P<sub>jip</sub>  作为标签排列，L<sub>c</sub>表示交叉熵： \n\n<img src=\"HLA-Face阅读笔记/image-20210905192310760.png\" alt=\"image-20210905192310760\" style=\"zoom:50%;\" />\n\nF<sub>jip</sub>表示相应的域提取的特征图，E(L)和H共享分类头，可以迫使语义特征映射到相同的空间中，从而缩小high-level之间的差距。最后拉近E(L)和H的损失为：\n\n<img src=\"HLA-Face阅读笔记/image-20210905192726753.png\" alt=\"image-20210905192726753\" style=\"zoom:50%;\" />\n\n**Closing H and D(H)**：对比学习的思想是给定一个 v，识别和它相似的样本v<sup>+</sup>，和它不相似的样本v<sup>-</sup>。利用点积来衡量相似性。目标损失为L<sub>q</sub>(v,v<sup>-</sup>,v<sup>+</sup>)。τ是个温度超参，直观的说，这是一个N+1分类的问题。为了减少H和D(H)之间的距离，作者利用对比学习使正样本更接近。这种方式可以提高H和D(H)之间的特性相似性，拉近high-level之间的差距。\n\n<img src=\"HLA-Face阅读笔记/image-20210905194211096.png\" alt=\"image-20210905194211096\" style=\"zoom:50%;\" />\n\n<img src=\"HLA-Face阅读笔记/image-20210905195112074.png\" alt=\"image-20210905195112074\" style=\"zoom:50%;\" />\n\n在H和D(H)引入单域对比学习，D(•)表示增广的一部分。简化后损失为公式9，D<sup>*</sup>(H)有50%的概率为H，有50%的概率为D(H)：\n\n<img src=\"HLA-Face阅读笔记/image-20210905195532167.png\" alt=\"image-20210905195532167\" style=\"zoom:50%;\" />\n\n**Enhancing E(L)**：增强E(L)同样也使用对比学习。\n\n<img src=\"HLA-Face阅读笔记/image-20210905195923509.png\" alt=\"image-20210905195923509\" style=\"zoom:50%;\" />\n\n**Final objective**：最终的损失函数。\n\n<img src=\"HLA-Face阅读笔记/image-20210905202018226.png\" alt=\"image-20210905202018226\" style=\"zoom:50%;\" />\n\n### 实验结果\n\n和当前的一些人脸检测模型、以及基于enhancement, darkening, and feature adaptation 三类方式的适应方式进行对比。\n\n![image-20210905203317458](HLA-Face阅读笔记/image-20210905203317458.png)\n\n\n\n参文：\n\n[15]Zero-referencedeep curve estimation for low-light image enhancement  \n","slug":"HLA-Face阅读笔记","published":1,"updated":"2021-09-05T13:25:32.269Z","comments":1,"photos":[],"link":"","_id":"cl7rqq9go0001jsvaamjq8wuo","content":"<p><a href=\"https://arxiv.org/abs/2104.01984v1\">HLA-Face: Joint High-Low Adaptation for Low Light Face Detection</a> 收录于CVPR2021，提升弱光环境人脸的检出率。</p>\n<p><a href=\"https://daooshee.github.io/HLA-Face-Website/\">源代码</a></p>\n<h3 id=\"摘要\">摘要</h3>\n<p>弱光环境下的人脸检测在现实中有着非常重要的应用，如夜间监控视频和自动驾驶等。目前多数人脸检测强依赖于大量的标注，而制作标准数据是非常费时费力的，为了减少制作弱光条件下建立新数据的负担，作者利用现有的正常光下的数据，探索如何适应正常光到弱光下的人脸检测。该任务的难点在于：正常光场景和弱光场景存在巨大且复杂的差异，现有的弱光增强和自适应方式不能达到很好的效果，因为，作者提出了一种联合高低适应（High-Low Adaptation ，HLA) ）框架，具体地，通过一个双向的low-level适应和多任务high-level适应，HLA在没有弱光标签进行训练的情况下，达到了最优效果。<span id=\"more\"></span></p>\n<h3 id=\"引言\">引言</h3>\n<p>人脸检测是许多计算机视觉任务的基础，已广泛应用到实际生活中，如智能监控、人脸解锁以及手机美颜。在过去几十年里，人脸检测研究取得了很大的进展，然而对于不良光照下的场景，很好的检出人脸依旧很困难。在弱光条件下拍摄的图片会出现一系列的退化：能见度低、强烈的噪声和色彩失真。这些退化不仅会影响人类的视觉能力，还会使机器视觉任务性能恶化。</p>\n<p>弱光数据DARK FACE的提出催生了大量了弱光人脸检测研究。然而，目前的研究依赖于大量的标注，因此有较差的鲁棒性。</p>\n<p>本文作者提出在不需要DARK FACE标注的情况的下，将正常光下的人脸检测模型适应于弱光场景中。作者发现正常光场景和弱光场景存在两个层次的差异：(1)像素级别：不足光照强度、相机噪音、颜色偏差。（2）对象语义级别：街灯、汽车灯、广告牌等。传统的弱光增强方法是为了提高视觉质量而设计的，无法填补语义上的差异。为了人脸检测模型从正常光场景适应到弱光场景下，作者考虑了low-level和high-level的联合适应，提出了HLA框架。具体来说，对于low-level适应，典型方法要么使图像变亮，要么使图像变暗，然而这种单向的从low-to-normal 和从normal-to-low的转换不能达到理想的效果，因此提出让两个域相互靠近一步，通过提高弱光图像的亮度和扭曲正常光图像，建立了介于正常和弱光之间的中间状态。对于high-level自适应，使用多任务自监督学习来缩小由low-level自适应构建的中间状态的特征距离。通过high-level和low-level适应，即时不使用弱光数据标签进行训练，本文方法也比当前最好的人脸检测模型表现更好。总得贡献如下：</p>\n<ul>\n<li>\n<p>通过联合high-level和low-level自适应，提出一种不使用弱光标签的弱光人脸检测框架，实现了当前最好性能。</p>\n</li>\n<li>\n<p>对于low-level自适应，设计了一个双向适应思路，对弱光数据进行增强，对正常光数据加入噪声和颜色扭曲色彩，建立中间状态，使两个域相互靠近。</p>\n</li>\n<li>\n<p>对于high-level自适应，引入跨域自监督学习进行特征自适应，通过基于上下文和对比学习，逼近多个域之间的特征距离，增强特征表达能力。</p>\n</li>\n</ul>\n<h3 id=\"弱光检测联合自适应\">弱光检测联合自适应</h3>\n<h4 id=\"动机\">动机</h4>\n<p>该任务是将在正常光数据（H）上训练的人脸检测网络适应到未被标注的弱光数据（L）上。如下图所示，现有方法大致分为enhancement, darkening, and feature adaptation  三类。基于enhancement的方式首先增亮弱光图像，然后直接对其进行测试，这类方法通常不需要微调模型，因此非常的灵活。基于darkening的方式首先将正常光的数据变暗，然后重新训练模型。基于特征自适应的方式大多使用对齐、对抗学习或伪标记直接自适应模型特征。</p>\n<h3 id=\"img-src-HLA-Face阅读笔记-image-20210905153749362-png-alt-image-20210905153749362-style-zoom-80\"><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905153749362.png\" alt=\"image-20210905153749362\" style=\"zoom:80%;\"></h3>\n<p>暗光人脸检测的主要问题是H和L之间存在复杂且巨大的差异，目前方法很难处理。在WIDER FACE和DARK FACE中的数据不仅有不同的像素级差异：（明亮v黑暗，干净vs噪声），还包含不同的物体和场景（绘画vs街景）。然而enhancement, darkening只考虑了像素级的差异，Feature adaptation  方法尝试一步填补整个差异，但是效果有限。</p>\n<p>对此，作者提出HLA方案，如上图（e）所示，作者设置了L和H之间的中间状态，并根据这些状态自适应相应的高级表达。具体地说，通过增强和暗化来减少H和L之间的low-level距离，相比单向的L-&gt;H或H-&gt;L，作者提出的双向转换L-&gt;E(L)和H-&gt; d (H)，更能缓解自适应的难度。然后通过将多个状态的特征空间相互推近来减少high-level之间的距离。在测试时，仅对图像进行L-&gt;E(L)的操作，然后用适应后的人脸检测器对E(L)进行测试。</p>\n<h4 id=\"双向low-level适应\">双向low-level适应</h4>\n<p>弱光降解是一个复杂的过程，作者将相关因素分为三个：光照、噪声和颜色失真。虽然去燥是颜色校正是非常困难的，但是添加噪声和扭曲颜色是比较容易的。在此基础上，将L进行亮化变成E(L)，对H加入噪声和扭曲颜色使其变成D(H)，与L和H相比，E(L)和D(H)更加相似。这样，我们就可以让H和L彼此走进一步，从而减轻自适应的困难。</p>\n<p><strong>Brightening</strong>  ：与普通的弱光增强任务不同，这里作者想要在不去噪和颜色校正的情况下调整，此外，弱光图像受到不均匀照明的影响，有些人脸被路灯照亮，而有些人脸被黑暗笼罩。因此，还需要防止过度曝光和曝光不足。基于非线性曲线映射[15]，由迭代二次曲线LE(·)构成。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905162418404.png\" alt=\"image-20210905162418404\" style=\"zoom: 50%;\">\n<p>LE<sub>o</sub> 是输入图片，LE<sub>n</sub> 是迭代n次的结果。A<sub>n</sub> 是由神经网络估计的像素级别的三通道映射图。与常见的端到端或基于Retinex理论的深度增强方式，曲线映射[15]不会引入额外的噪声或伪影。直接按照[15]的方式进行弱光增强力度不够，很多人脸仍被黑暗笼罩，这是因为过多的增加进行增强会引入噪声。而在本文中，作者为了更多的提高图像亮度，将公式（1）中的迭代次数加倍和扩大曲线估计网络。虽然引入了噪声和使眼色失真，但是可以通过H-&gt;D(H)处理。</p>\n<p><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905184019018.png\" alt=\"image-20210905184019018\"></p>\n<p><strong>Noise Synthesis</strong>：虽然可以通过Brightening减少像素之间的差距，但是E(L)和H之间的差距仍然具有挑战性。因此，作者将剩下的差距进一步分解为颜色和噪声。同时，通过分离颜色，可以利用颜色去引导噪声合成。</p>\n<p>如上图所示，首先利用一个双边滤波器对E(L)进行模糊化，然后利用Pix2Pix网络训练E(L)<sub>blur</sub> -&gt;E(L)，这样就得到了一个添加噪声的网络。最后利用训练好的Pix2Pix网络对H进行添加噪声。H<sub>noise</sub> 成功的模拟了E(L)的噪声。</p>\n<p><strong>Color Jittering</strong>：作者希望D(H)和E(L)之间有着相似的颜色分布，基于静态统计，作者设置了亮度、对比度、饱和度、色度的抖动范围分别在（0.4，1.2）、（0.6，1.4）、（0.6，1.4）和（0.8，1.2）。</p>\n<h4 id=\"多任务high-level适应\">多任务high-level适应</h4>\n<p>特征自适应方法大多是基于对齐、伪标记和对抗学习，然而，对齐和伪标记不能很好的处理巨大的差距，而对抗学习不稳定。作者提出充分利用图像本身的自然信息，通过跨域的自监督分类学习，特征被迫映射到相同的高维子空间，从而缩小high-level之间的特征差距。</p>\n<p>为了使E(L)、H和D(H)之间相互靠近，首先通过基于上下文的跨域自监督学习使E(L)和H之间靠近，然后通过跨域对比学习拉近H和D(H)，还通过单域对比学习进一步增强E(L)的表示。以多任务的方式实现整个自适应工作。</p>\n<p><strong>Closing E(L) and H</strong>：基于上下文自监督设计代理任务（pretext tasks），可以使模型学习和理解目标的空间背景信息。在这里，作者使用拼图游戏，作者指出旋转和结合旋转拼图，效果不如单独使用拼图。可能原因，在WIDER FACE数据中，很多图像都是绘画和广告，其中的脸可能有很奇怪的角度，因此旋转预测代理任务可能是模糊不清的。作者将3x3的图像块组成整幅图像，设置图像排列为30，P<sub>jip</sub>  作为标签排列，L<sub>c</sub>表示交叉熵：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905192310760.png\" alt=\"image-20210905192310760\" style=\"zoom:50%;\">\n<p>F<sub>jip</sub>表示相应的域提取的特征图，E(L)和H共享分类头，可以迫使语义特征映射到相同的空间中，从而缩小high-level之间的差距。最后拉近E(L)和H的损失为：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905192726753.png\" alt=\"image-20210905192726753\" style=\"zoom:50%;\">\n<p><strong>Closing H and D(H)</strong>：对比学习的思想是给定一个 v，识别和它相似的样本v<sup>+</sup>，和它不相似的样本v<sup>-</sup>。利用点积来衡量相似性。目标损失为L<sub>q</sub>(v,v<sup>-</sup>,v<sup>+</sup>)。τ是个温度超参，直观的说，这是一个N+1分类的问题。为了减少H和D(H)之间的距离，作者利用对比学习使正样本更接近。这种方式可以提高H和D(H)之间的特性相似性，拉近high-level之间的差距。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905194211096.png\" alt=\"image-20210905194211096\" style=\"zoom:50%;\">\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195112074.png\" alt=\"image-20210905195112074\" style=\"zoom:50%;\">\n<p>在H和D(H)引入单域对比学习，D(•)表示增广的一部分。简化后损失为公式9，D<sup>*</sup>(H)有50%的概率为H，有50%的概率为D(H)：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195532167.png\" alt=\"image-20210905195532167\" style=\"zoom:50%;\">\n<p><strong>Enhancing E(L)</strong>：增强E(L)同样也使用对比学习。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195923509.png\" alt=\"image-20210905195923509\" style=\"zoom:50%;\">\n<p><strong>Final objective</strong>：最终的损失函数。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905202018226.png\" alt=\"image-20210905202018226\" style=\"zoom:50%;\">\n<h3 id=\"实验结果\">实验结果</h3>\n<p>和当前的一些人脸检测模型、以及基于enhancement, darkening, and feature adaptation 三类方式的适应方式进行对比。</p>\n<p><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905203317458.png\" alt=\"image-20210905203317458\"></p>\n<p>参文：</p>\n<p>[15]Zero-referencedeep curve estimation for low-light image enhancement</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"https://arxiv.org/abs/2104.01984v1\">HLA-Face: Joint High-Low Adaptation for Low Light Face Detection</a> 收录于CVPR2021，提升弱光环境人脸的检出率。</p>\n<p><a href=\"https://daooshee.github.io/HLA-Face-Website/\">源代码</a></p>\n<h3 id=\"摘要\">摘要</h3>\n<p>弱光环境下的人脸检测在现实中有着非常重要的应用，如夜间监控视频和自动驾驶等。目前多数人脸检测强依赖于大量的标注，而制作标准数据是非常费时费力的，为了减少制作弱光条件下建立新数据的负担，作者利用现有的正常光下的数据，探索如何适应正常光到弱光下的人脸检测。该任务的难点在于：正常光场景和弱光场景存在巨大且复杂的差异，现有的弱光增强和自适应方式不能达到很好的效果，因为，作者提出了一种联合高低适应（High-Low Adaptation ，HLA) ）框架，具体地，通过一个双向的low-level适应和多任务high-level适应，HLA在没有弱光标签进行训练的情况下，达到了最优效果。</p>","more":"<p></p>\n<h3 id=\"引言\">引言</h3>\n<p>人脸检测是许多计算机视觉任务的基础，已广泛应用到实际生活中，如智能监控、人脸解锁以及手机美颜。在过去几十年里，人脸检测研究取得了很大的进展，然而对于不良光照下的场景，很好的检出人脸依旧很困难。在弱光条件下拍摄的图片会出现一系列的退化：能见度低、强烈的噪声和色彩失真。这些退化不仅会影响人类的视觉能力，还会使机器视觉任务性能恶化。</p>\n<p>弱光数据DARK FACE的提出催生了大量了弱光人脸检测研究。然而，目前的研究依赖于大量的标注，因此有较差的鲁棒性。</p>\n<p>本文作者提出在不需要DARK FACE标注的情况的下，将正常光下的人脸检测模型适应于弱光场景中。作者发现正常光场景和弱光场景存在两个层次的差异：(1)像素级别：不足光照强度、相机噪音、颜色偏差。（2）对象语义级别：街灯、汽车灯、广告牌等。传统的弱光增强方法是为了提高视觉质量而设计的，无法填补语义上的差异。为了人脸检测模型从正常光场景适应到弱光场景下，作者考虑了low-level和high-level的联合适应，提出了HLA框架。具体来说，对于low-level适应，典型方法要么使图像变亮，要么使图像变暗，然而这种单向的从low-to-normal 和从normal-to-low的转换不能达到理想的效果，因此提出让两个域相互靠近一步，通过提高弱光图像的亮度和扭曲正常光图像，建立了介于正常和弱光之间的中间状态。对于high-level自适应，使用多任务自监督学习来缩小由low-level自适应构建的中间状态的特征距离。通过high-level和low-level适应，即时不使用弱光数据标签进行训练，本文方法也比当前最好的人脸检测模型表现更好。总得贡献如下：</p>\n<ul>\n<li>\n<p>通过联合high-level和low-level自适应，提出一种不使用弱光标签的弱光人脸检测框架，实现了当前最好性能。</p>\n</li>\n<li>\n<p>对于low-level自适应，设计了一个双向适应思路，对弱光数据进行增强，对正常光数据加入噪声和颜色扭曲色彩，建立中间状态，使两个域相互靠近。</p>\n</li>\n<li>\n<p>对于high-level自适应，引入跨域自监督学习进行特征自适应，通过基于上下文和对比学习，逼近多个域之间的特征距离，增强特征表达能力。</p>\n</li>\n</ul>\n<h3 id=\"弱光检测联合自适应\">弱光检测联合自适应</h3>\n<h4 id=\"动机\">动机</h4>\n<p>该任务是将在正常光数据（H）上训练的人脸检测网络适应到未被标注的弱光数据（L）上。如下图所示，现有方法大致分为enhancement, darkening, and feature adaptation  三类。基于enhancement的方式首先增亮弱光图像，然后直接对其进行测试，这类方法通常不需要微调模型，因此非常的灵活。基于darkening的方式首先将正常光的数据变暗，然后重新训练模型。基于特征自适应的方式大多使用对齐、对抗学习或伪标记直接自适应模型特征。</p>\n<h3 id=\"img-src-HLA-Face阅读笔记-image-20210905153749362-png-alt-image-20210905153749362-style-zoom-80\"><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905153749362.png\" alt=\"image-20210905153749362\" style=\"zoom:80%;\"></h3>\n<p>暗光人脸检测的主要问题是H和L之间存在复杂且巨大的差异，目前方法很难处理。在WIDER FACE和DARK FACE中的数据不仅有不同的像素级差异：（明亮v黑暗，干净vs噪声），还包含不同的物体和场景（绘画vs街景）。然而enhancement, darkening只考虑了像素级的差异，Feature adaptation  方法尝试一步填补整个差异，但是效果有限。</p>\n<p>对此，作者提出HLA方案，如上图（e）所示，作者设置了L和H之间的中间状态，并根据这些状态自适应相应的高级表达。具体地说，通过增强和暗化来减少H和L之间的low-level距离，相比单向的L-&gt;H或H-&gt;L，作者提出的双向转换L-&gt;E(L)和H-&gt; d (H)，更能缓解自适应的难度。然后通过将多个状态的特征空间相互推近来减少high-level之间的距离。在测试时，仅对图像进行L-&gt;E(L)的操作，然后用适应后的人脸检测器对E(L)进行测试。</p>\n<h4 id=\"双向low-level适应\">双向low-level适应</h4>\n<p>弱光降解是一个复杂的过程，作者将相关因素分为三个：光照、噪声和颜色失真。虽然去燥是颜色校正是非常困难的，但是添加噪声和扭曲颜色是比较容易的。在此基础上，将L进行亮化变成E(L)，对H加入噪声和扭曲颜色使其变成D(H)，与L和H相比，E(L)和D(H)更加相似。这样，我们就可以让H和L彼此走进一步，从而减轻自适应的困难。</p>\n<p><strong>Brightening</strong>  ：与普通的弱光增强任务不同，这里作者想要在不去噪和颜色校正的情况下调整，此外，弱光图像受到不均匀照明的影响，有些人脸被路灯照亮，而有些人脸被黑暗笼罩。因此，还需要防止过度曝光和曝光不足。基于非线性曲线映射[15]，由迭代二次曲线LE(·)构成。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905162418404.png\" alt=\"image-20210905162418404\" style=\"zoom: 50%;\">\n<p>LE<sub>o</sub> 是输入图片，LE<sub>n</sub> 是迭代n次的结果。A<sub>n</sub> 是由神经网络估计的像素级别的三通道映射图。与常见的端到端或基于Retinex理论的深度增强方式，曲线映射[15]不会引入额外的噪声或伪影。直接按照[15]的方式进行弱光增强力度不够，很多人脸仍被黑暗笼罩，这是因为过多的增加进行增强会引入噪声。而在本文中，作者为了更多的提高图像亮度，将公式（1）中的迭代次数加倍和扩大曲线估计网络。虽然引入了噪声和使眼色失真，但是可以通过H-&gt;D(H)处理。</p>\n<p><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905184019018.png\" alt=\"image-20210905184019018\"></p>\n<p><strong>Noise Synthesis</strong>：虽然可以通过Brightening减少像素之间的差距，但是E(L)和H之间的差距仍然具有挑战性。因此，作者将剩下的差距进一步分解为颜色和噪声。同时，通过分离颜色，可以利用颜色去引导噪声合成。</p>\n<p>如上图所示，首先利用一个双边滤波器对E(L)进行模糊化，然后利用Pix2Pix网络训练E(L)<sub>blur</sub> -&gt;E(L)，这样就得到了一个添加噪声的网络。最后利用训练好的Pix2Pix网络对H进行添加噪声。H<sub>noise</sub> 成功的模拟了E(L)的噪声。</p>\n<p><strong>Color Jittering</strong>：作者希望D(H)和E(L)之间有着相似的颜色分布，基于静态统计，作者设置了亮度、对比度、饱和度、色度的抖动范围分别在（0.4，1.2）、（0.6，1.4）、（0.6，1.4）和（0.8，1.2）。</p>\n<h4 id=\"多任务high-level适应\">多任务high-level适应</h4>\n<p>特征自适应方法大多是基于对齐、伪标记和对抗学习，然而，对齐和伪标记不能很好的处理巨大的差距，而对抗学习不稳定。作者提出充分利用图像本身的自然信息，通过跨域的自监督分类学习，特征被迫映射到相同的高维子空间，从而缩小high-level之间的特征差距。</p>\n<p>为了使E(L)、H和D(H)之间相互靠近，首先通过基于上下文的跨域自监督学习使E(L)和H之间靠近，然后通过跨域对比学习拉近H和D(H)，还通过单域对比学习进一步增强E(L)的表示。以多任务的方式实现整个自适应工作。</p>\n<p><strong>Closing E(L) and H</strong>：基于上下文自监督设计代理任务（pretext tasks），可以使模型学习和理解目标的空间背景信息。在这里，作者使用拼图游戏，作者指出旋转和结合旋转拼图，效果不如单独使用拼图。可能原因，在WIDER FACE数据中，很多图像都是绘画和广告，其中的脸可能有很奇怪的角度，因此旋转预测代理任务可能是模糊不清的。作者将3x3的图像块组成整幅图像，设置图像排列为30，P<sub>jip</sub>  作为标签排列，L<sub>c</sub>表示交叉熵：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905192310760.png\" alt=\"image-20210905192310760\" style=\"zoom:50%;\">\n<p>F<sub>jip</sub>表示相应的域提取的特征图，E(L)和H共享分类头，可以迫使语义特征映射到相同的空间中，从而缩小high-level之间的差距。最后拉近E(L)和H的损失为：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905192726753.png\" alt=\"image-20210905192726753\" style=\"zoom:50%;\">\n<p><strong>Closing H and D(H)</strong>：对比学习的思想是给定一个 v，识别和它相似的样本v<sup>+</sup>，和它不相似的样本v<sup>-</sup>。利用点积来衡量相似性。目标损失为L<sub>q</sub>(v,v<sup>-</sup>,v<sup>+</sup>)。τ是个温度超参，直观的说，这是一个N+1分类的问题。为了减少H和D(H)之间的距离，作者利用对比学习使正样本更接近。这种方式可以提高H和D(H)之间的特性相似性，拉近high-level之间的差距。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905194211096.png\" alt=\"image-20210905194211096\" style=\"zoom:50%;\">\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195112074.png\" alt=\"image-20210905195112074\" style=\"zoom:50%;\">\n<p>在H和D(H)引入单域对比学习，D(•)表示增广的一部分。简化后损失为公式9，D<sup>*</sup>(H)有50%的概率为H，有50%的概率为D(H)：</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195532167.png\" alt=\"image-20210905195532167\" style=\"zoom:50%;\">\n<p><strong>Enhancing E(L)</strong>：增强E(L)同样也使用对比学习。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905195923509.png\" alt=\"image-20210905195923509\" style=\"zoom:50%;\">\n<p><strong>Final objective</strong>：最终的损失函数。</p>\n<img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905202018226.png\" alt=\"image-20210905202018226\" style=\"zoom:50%;\">\n<h3 id=\"实验结果\">实验结果</h3>\n<p>和当前的一些人脸检测模型、以及基于enhancement, darkening, and feature adaptation 三类方式的适应方式进行对比。</p>\n<p><img src=\"/2021/09/04/HLA-Face%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20210905203317458.png\" alt=\"image-20210905203317458\"></p>\n<p>参文：</p>\n<p>[15]Zero-referencedeep curve estimation for low-light image enhancement</p>"},{"title":"VS code环境设置","date":"2022-08-29T08:07:57.000Z","_content":"\n### 给VS code 配置临时环境\n\n经常用vs code开放python项目，如果调试远程服务器的项目时，常用做法是在终端配置环境变量，然后在终端运行程序，通过 pdb 库调试，查看中间结果，个人觉得很不方便，本文介绍一下通过launch.json配置环境，可以进行debug调试\n\n<!--more-->\n\n```python\n{\n    \"veersion\": \"0.2.0\",\n    \"configurations\":[\n        {\n            \"name\": \"Python:Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\"： \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"env\": {\n                \"LD_LIBRARY_PATH\": \"xxxxxxxxxxxxxx\"\n                \"MXNET_CUDNN_AUTOTUNE_DEFAULT\": \"0\"\n            },\n        }\n    ]\n}\n```\n\n 在项目下有launch.json文件，在env中设置我们需要的环境变量，\n\n","source":"_posts/VS-code环境设置.md","raw":"---\ntitle: VS code环境设置\ndate: 2022-08-29 16:07:57\ntags:\ncategories:\n- 工具\n---\n\n### 给VS code 配置临时环境\n\n经常用vs code开放python项目，如果调试远程服务器的项目时，常用做法是在终端配置环境变量，然后在终端运行程序，通过 pdb 库调试，查看中间结果，个人觉得很不方便，本文介绍一下通过launch.json配置环境，可以进行debug调试\n\n<!--more-->\n\n```python\n{\n    \"veersion\": \"0.2.0\",\n    \"configurations\":[\n        {\n            \"name\": \"Python:Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\"： \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"env\": {\n                \"LD_LIBRARY_PATH\": \"xxxxxxxxxxxxxx\"\n                \"MXNET_CUDNN_AUTOTUNE_DEFAULT\": \"0\"\n            },\n        }\n    ]\n}\n```\n\n 在项目下有launch.json文件，在env中设置我们需要的环境变量，\n\n","slug":"VS-code环境设置","published":1,"updated":"2022-09-07T05:51:52.708Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl7rqq9gu0002jsva31b1833i","content":"<h3 id=\"给VS-code-配置临时环境\">给VS code 配置临时环境</h3>\n<p>经常用vs code开放python项目，如果调试远程服务器的项目时，常用做法是在终端配置环境变量，然后在终端运行程序，通过 pdb 库调试，查看中间结果，个人觉得很不方便，本文介绍一下通过launch.json配置环境，可以进行debug调试</p>\n<span id=\"more\"></span>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;veersion&quot;</span>: <span class=\"string\">&quot;0.2.0&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;configurations&quot;</span>:[</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;Python:Current File&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;python&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;request&quot;</span>: <span class=\"string\">&quot;launch&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;program&quot;</span>： <span class=\"string\">&quot;$&#123;file&#125;&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;console&quot;</span>: <span class=\"string\">&quot;integratedTerminal&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;env&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;LD_LIBRARY_PATH&quot;</span>: <span class=\"string\">&quot;xxxxxxxxxxxxxx&quot;</span></span><br><span class=\"line\">                <span class=\"string\">&quot;MXNET_CUDNN_AUTOTUNE_DEFAULT&quot;</span>: <span class=\"string\">&quot;0&quot;</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在项目下有launch.json文件，在env中设置我们需要的环境变量，</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"给VS-code-配置临时环境\">给VS code 配置临时环境</h3>\n<p>经常用vs code开放python项目，如果调试远程服务器的项目时，常用做法是在终端配置环境变量，然后在终端运行程序，通过 pdb 库调试，查看中间结果，个人觉得很不方便，本文介绍一下通过launch.json配置环境，可以进行debug调试</p>","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;veersion&quot;</span>: <span class=\"string\">&quot;0.2.0&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;configurations&quot;</span>:[</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;Python:Current File&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;python&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;request&quot;</span>: <span class=\"string\">&quot;launch&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;program&quot;</span>： <span class=\"string\">&quot;$&#123;file&#125;&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;console&quot;</span>: <span class=\"string\">&quot;integratedTerminal&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;env&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;LD_LIBRARY_PATH&quot;</span>: <span class=\"string\">&quot;xxxxxxxxxxxxxx&quot;</span></span><br><span class=\"line\">                <span class=\"string\">&quot;MXNET_CUDNN_AUTOTUNE_DEFAULT&quot;</span>: <span class=\"string\">&quot;0&quot;</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在项目下有launch.json文件，在env中设置我们需要的环境变量，</p>"},{"title":"目标检测中的NMS","date":"2021-06-12T17:06:06.000Z","top":5,"_content":"\n最近一直在做人脸检测的项目，其中有一个重要过程就是剔除冗余的目标框，该过程利用NMS(Non-Maximum Suppression)方法，最近整理出本人对NMS的理解内容，以及基于Python的实现，如有不对请指出。:fallen_leaf::fallen_leaf::fallen_leaf:\n\n非极大值抑制(Non-Maximum Suppression, NMS),就是抑制不是极大值的元素，多用于目标检测中。一般来说，在目标检测算法输出目标框时，目标框会非常多，其中会有很多重复的框定位到同一个目标上，NMS的作用就是去除冗余框，为每一个目标得到唯一的检测框，<!--more-->如下图所示。\n\n![fig.1](目标检测中的NMS/blog1_fig_1.png)\n\n\n\n### 1、标准的NMS\n标准NMS的做法将检测框按得分排序，然后保留得分最高的框，同时删除与该框重叠面积大于一定阈值的其他框。具体的算法流程：\n\n![fig.2](目标检测中的NMS/blog1_fig_5.png)\n\nPython实现：\n\n```python\ndef py_cpu_nms(dets,thresh):\n    '''\n    dets：[[xmin,ymin,xmax,ymax,scores],...]\n    thresh：iou 阈值\n    '''\n    x1=dets[:,0]\n    y1=dets[:,1]\n    x2=dets[:,2]\n    y2=dets[:,3]\n    areas=(x2-x1+1)*(y2-y1+1)\n    scores=dets[:,4]\n\n    keep=[]  #存放nms剩余的方框\n\n    index=scores.argsort()[::-1]\n    print(index)\n    while index.size>0:\n        print(index.size)\n        i=index[0]\n        keep.append(i)\n        \n        x11 = np.maximum(x1[i], x1[index[1:]])    # calculate the points of overlap \n        y11 = np.maximum(y1[i], y1[index[1:]])\n        x22 = np.minimum(x2[i], x2[index[1:]])\n        y22 = np.minimum(y2[i], y2[index[1:]])\n        \n        w=np.maximum(0,x22-x11+1)\n        h=np.maximum(0,y22-x11+1)\n\n        overlaps = w*h\n\n        ious=overlaps / (areas[i]+areas[index[1:]]-overlaps)        \n        print(\"ious is :\",ious)\n\n        idx=np.where(ious<thresh)[0]\n\n        index=index[idx+1]\n        print(index)\n    return keep\n\n```\n### 2、soft NMS：\n论文：[Improving Object Detection With One Line of Code](http://cn.arxiv.org/abs/1704.04503)\n\n提出soft NMS的作者认为，标准NMS会存在如下图所示的问题，红色框和绿色框是当前的检测结果，二者的得分分别是0.95和0.80。如果按照传统的NMS进行处理，首先选中得分最高的红色框，然后绿色框就会因为与之重叠面积过大而被删掉。另一方面，NMS的阈值也不太容易确定，设小了会出现下图的情况（绿色框因为和红色框重叠面积较大而被删掉），设置过高又容易增大误检。​​\n\n![](./目标检测中的NMS/blog1_fig_3.jpg)\n\nsoft NMS思想：不要删除所有IOU大于阈值的框，而是降低其置信度。具体的算法流程：\n\n![](./目标检测中的NMS/blog1_fig_4.jpg)\n\nPython实现：\n```python\ndef cpu_soft_nms(boxes, sigma=0.5, Nt=0.3, threshold=0.1, method=2):\n    '''\n    boxes：[[xmin,ymin,xmax,ymax,scores],...]\n    '''\n    N = boxes.shape[0]\n \n    pos = 0\n    maxscore = 0\n    maxpos = 0\n\n\n    for i in range(N):\n        # 用冒泡排序法找到分数最高的预测框，并将该预测框放在第i个位置\n        maxscore = boxes[i, 4]\n        maxpos = i\n\n        # 先用一些中间变量存储第i个预测框\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = boxes[i,4]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < boxes[pos, 4]:\n                maxscore = boxes[pos, 4]\n                maxpos = pos\n            pos = pos + 1\n\n\t    # 将分数最高的预测框M放在第i个位置\n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        boxes[i,4] = boxes[maxpos,4]\n\n\t    # 将原先第i个预测框放在分数最高的位置\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        boxes[maxpos,4] = ts\n\n        # 程序到此实现了：寻找第i至第N个预测框中分数最高的框，并将其与第i个预测框互换位置。\n\n        # 预测框M，前缀\"t\"表示target\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = boxes[i,4]\n\n\t    # 下面针对M进行NMS迭代过程，\n        # 需要注意的是，如果soft-NMS将score削弱至某阈值threshold以下，则将其删除掉\n        # 在程序中体现为，将要删除的框放在了最后，并使 N = N-1\n        pos = i + 1\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = boxes[pos, 4]\n\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                if ih > 0:\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    ov = iw * ih / ua  # iou between max box and detection box\n\n                    if method == 1:  # linear\n                        if ov > Nt: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2:  # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else:  # original NMS\n                        if ov > Nt: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    boxes[pos, 4] = weight*boxes[pos, 4]\n\t\t    \n\t\t            # if box score falls below threshold, discard the box by swapping with last box\n\t\t            # update N\n                    if boxes[pos, 4] < threshold:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        boxes[pos,4] = boxes[N-1, 4]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n\n    keep = [i for i in range(N)]\n    return keep\n```\n\n### 3、weighted NMS：\n加权NMS认为传统NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框（得分不是最高的框）也有可能是定位良好的。那么与直接剔除机制不同而是根据网络预测的置信度进行加权，得到新的目标框，把该目标框作为最终的目标框，再将其他框剔除。\n具体的算法流程：\n\n<!-- ![](./目标检测中的NMS/blog1_fig_4.jpg) -->\n\nPython实现：\n```python\ndef py_weighted_nms(dets, thresh):\n    \"\"\"\n    Takes bounding boxes and scores and a threshold and applies \n    weighted non-maximal suppression.\n        \n    dets：[[xmin,ymin,xmax,ymax,scores],...]\n    thresh：iou 阈值\n    \"\"\"\n    scores = dets[:, 4]\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n\n    areas = (x2 - x1) * (y2 - y1)\n    order = scores.argsort()[::-1]\n    dets =dets[order]\n    max_ids = []\n    weighted_boxes = []\n    while order.size > 0:\n        i = order[0]\n        max_ids.append(i)\n        xx1 = np.maximum(x1[i], x1[order[:]])\n        yy1 = np.maximum(y1[i], y1[order[:]])\n        xx2 = np.minimum(x2[i], x2[order[:]])\n        yy2 = np.minimum(y2[i], y2[order[:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[:]] - inter)\n\n        in_inds = np.where(iou >= thresh)[0]\n        \n        in_dets = dets[in_inds, :]\n\n        weights = in_dets[:, 4] * iou[in_inds]\n        wbox = np.sum((in_dets[:, :4] * weights[..., np.newaxis]), axis=0) \\\n            / np.sum(weights)\n        weighted_boxes.append(wbox)\n\n        out_inds = np.where(iou < thresh)[0]\n        order = order[out_inds]\n        dets = dets[out_inds]\n    scores_frinally=scores[max_ids]\n    return max_ids,scores_frinally, np.array(weighted_boxes)\n```\n### 4、其他思路\n\n以上三种NMS方法都是针对目标检测的效果而设计的算法，在这些NMS算法的实现上首先都用到了排序。在查找资源时，偶然看到一种不需要排序的NMS方式（[浅谈NMS的多种实现](https://zhuanlan.zhihu.com/p/64423753)）。这里我简单介绍一下大佬的实现思路：依次遍历每个框，计算这个框与其他框的iou,找到iou大于一定阈值的其他框，因为这个时候不能保证它一定是score最高的框，所以要进行判断，如果它的score小于其他框，那就把它去掉，因为它肯定不是要保留的框。如果它的score大于其他框，那应该保留它，同时可以去掉所有其他框了。最后保留的框就是结果。其实这种思想就是针对\"标准NMS\"提出的不用排序的另一种解法。\n\npython实现：\n\n```python\ndef nms(bbox, scores, thresh):\n    area=np.prod(bbox[:,2:]-bbox[:,:2],axis=1)\n    keep=np.ones(len(bbox),dtype=bool)\n    for i, b in enumerate(bbox):\n        if(keep[i]==False):\n            continue\n        tl=np.maximum(b[:2],bbox[i+1:,:2])\n        br=np.minimum(b[2:],bbox[i+1:,2:])\n        inter=np.prod(br-tl,axis=1)*(br>=tl).all(axis=1)\n        iou=ia/(area[i+1:]+area[i]-inter)\n        r = [ k for k in np.where(iou>thresh)[0]+i+1 if keep[k]==True]\n        if (scores[i]>scores[r]).all():\n            keep[r]=False\n        else:\n            keep[i]=False\n    return np.where(keep)[0].astype(np.int32)\n```\n\n\n\n相关资源：\n\n[目标检测中的NMS](https://zhuanlan.zhihu.com/p/110256988)\n\n[目标检测中的NMS，soft NMS，softer NMS，Weighted Boxes Fusion](https://blog.csdn.net/practical_sharp/article/details/114980578)\n\n[浅谈NMS的多种实现](https://zhuanlan.zhihu.com/p/64423753)","source":"_posts/目标检测中的NMS.md","raw":"---\ntitle: 目标检测中的NMS\ndate: 2021-06-13 01:06:06\ntop: 5\ntag: 随笔\ncategories:\n- 图像处理\n- 目标检测\n# declare: true\n---\n\n最近一直在做人脸检测的项目，其中有一个重要过程就是剔除冗余的目标框，该过程利用NMS(Non-Maximum Suppression)方法，最近整理出本人对NMS的理解内容，以及基于Python的实现，如有不对请指出。:fallen_leaf::fallen_leaf::fallen_leaf:\n\n非极大值抑制(Non-Maximum Suppression, NMS),就是抑制不是极大值的元素，多用于目标检测中。一般来说，在目标检测算法输出目标框时，目标框会非常多，其中会有很多重复的框定位到同一个目标上，NMS的作用就是去除冗余框，为每一个目标得到唯一的检测框，<!--more-->如下图所示。\n\n![fig.1](目标检测中的NMS/blog1_fig_1.png)\n\n\n\n### 1、标准的NMS\n标准NMS的做法将检测框按得分排序，然后保留得分最高的框，同时删除与该框重叠面积大于一定阈值的其他框。具体的算法流程：\n\n![fig.2](目标检测中的NMS/blog1_fig_5.png)\n\nPython实现：\n\n```python\ndef py_cpu_nms(dets,thresh):\n    '''\n    dets：[[xmin,ymin,xmax,ymax,scores],...]\n    thresh：iou 阈值\n    '''\n    x1=dets[:,0]\n    y1=dets[:,1]\n    x2=dets[:,2]\n    y2=dets[:,3]\n    areas=(x2-x1+1)*(y2-y1+1)\n    scores=dets[:,4]\n\n    keep=[]  #存放nms剩余的方框\n\n    index=scores.argsort()[::-1]\n    print(index)\n    while index.size>0:\n        print(index.size)\n        i=index[0]\n        keep.append(i)\n        \n        x11 = np.maximum(x1[i], x1[index[1:]])    # calculate the points of overlap \n        y11 = np.maximum(y1[i], y1[index[1:]])\n        x22 = np.minimum(x2[i], x2[index[1:]])\n        y22 = np.minimum(y2[i], y2[index[1:]])\n        \n        w=np.maximum(0,x22-x11+1)\n        h=np.maximum(0,y22-x11+1)\n\n        overlaps = w*h\n\n        ious=overlaps / (areas[i]+areas[index[1:]]-overlaps)        \n        print(\"ious is :\",ious)\n\n        idx=np.where(ious<thresh)[0]\n\n        index=index[idx+1]\n        print(index)\n    return keep\n\n```\n### 2、soft NMS：\n论文：[Improving Object Detection With One Line of Code](http://cn.arxiv.org/abs/1704.04503)\n\n提出soft NMS的作者认为，标准NMS会存在如下图所示的问题，红色框和绿色框是当前的检测结果，二者的得分分别是0.95和0.80。如果按照传统的NMS进行处理，首先选中得分最高的红色框，然后绿色框就会因为与之重叠面积过大而被删掉。另一方面，NMS的阈值也不太容易确定，设小了会出现下图的情况（绿色框因为和红色框重叠面积较大而被删掉），设置过高又容易增大误检。​​\n\n![](./目标检测中的NMS/blog1_fig_3.jpg)\n\nsoft NMS思想：不要删除所有IOU大于阈值的框，而是降低其置信度。具体的算法流程：\n\n![](./目标检测中的NMS/blog1_fig_4.jpg)\n\nPython实现：\n```python\ndef cpu_soft_nms(boxes, sigma=0.5, Nt=0.3, threshold=0.1, method=2):\n    '''\n    boxes：[[xmin,ymin,xmax,ymax,scores],...]\n    '''\n    N = boxes.shape[0]\n \n    pos = 0\n    maxscore = 0\n    maxpos = 0\n\n\n    for i in range(N):\n        # 用冒泡排序法找到分数最高的预测框，并将该预测框放在第i个位置\n        maxscore = boxes[i, 4]\n        maxpos = i\n\n        # 先用一些中间变量存储第i个预测框\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = boxes[i,4]\n\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < boxes[pos, 4]:\n                maxscore = boxes[pos, 4]\n                maxpos = pos\n            pos = pos + 1\n\n\t    # 将分数最高的预测框M放在第i个位置\n        boxes[i,0] = boxes[maxpos,0]\n        boxes[i,1] = boxes[maxpos,1]\n        boxes[i,2] = boxes[maxpos,2]\n        boxes[i,3] = boxes[maxpos,3]\n        boxes[i,4] = boxes[maxpos,4]\n\n\t    # 将原先第i个预测框放在分数最高的位置\n        boxes[maxpos,0] = tx1\n        boxes[maxpos,1] = ty1\n        boxes[maxpos,2] = tx2\n        boxes[maxpos,3] = ty2\n        boxes[maxpos,4] = ts\n\n        # 程序到此实现了：寻找第i至第N个预测框中分数最高的框，并将其与第i个预测框互换位置。\n\n        # 预测框M，前缀\"t\"表示target\n        tx1 = boxes[i,0]\n        ty1 = boxes[i,1]\n        tx2 = boxes[i,2]\n        ty2 = boxes[i,3]\n        ts = boxes[i,4]\n\n\t    # 下面针对M进行NMS迭代过程，\n        # 需要注意的是，如果soft-NMS将score削弱至某阈值threshold以下，则将其删除掉\n        # 在程序中体现为，将要删除的框放在了最后，并使 N = N-1\n        pos = i + 1\n        while pos < N:\n            x1 = boxes[pos, 0]\n            y1 = boxes[pos, 1]\n            x2 = boxes[pos, 2]\n            y2 = boxes[pos, 3]\n            s = boxes[pos, 4]\n\n            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n            iw = (min(tx2, x2) - max(tx1, x1) + 1)\n            if iw > 0:\n                ih = (min(ty2, y2) - max(ty1, y1) + 1)\n                if ih > 0:\n                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)\n                    ov = iw * ih / ua  # iou between max box and detection box\n\n                    if method == 1:  # linear\n                        if ov > Nt: \n                            weight = 1 - ov\n                        else:\n                            weight = 1\n                    elif method == 2:  # gaussian\n                        weight = np.exp(-(ov * ov)/sigma)\n                    else:  # original NMS\n                        if ov > Nt: \n                            weight = 0\n                        else:\n                            weight = 1\n\n                    boxes[pos, 4] = weight*boxes[pos, 4]\n\t\t    \n\t\t            # if box score falls below threshold, discard the box by swapping with last box\n\t\t            # update N\n                    if boxes[pos, 4] < threshold:\n                        boxes[pos,0] = boxes[N-1, 0]\n                        boxes[pos,1] = boxes[N-1, 1]\n                        boxes[pos,2] = boxes[N-1, 2]\n                        boxes[pos,3] = boxes[N-1, 3]\n                        boxes[pos,4] = boxes[N-1, 4]\n                        N = N - 1\n                        pos = pos - 1\n\n            pos = pos + 1\n\n    keep = [i for i in range(N)]\n    return keep\n```\n\n### 3、weighted NMS：\n加权NMS认为传统NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框（得分不是最高的框）也有可能是定位良好的。那么与直接剔除机制不同而是根据网络预测的置信度进行加权，得到新的目标框，把该目标框作为最终的目标框，再将其他框剔除。\n具体的算法流程：\n\n<!-- ![](./目标检测中的NMS/blog1_fig_4.jpg) -->\n\nPython实现：\n```python\ndef py_weighted_nms(dets, thresh):\n    \"\"\"\n    Takes bounding boxes and scores and a threshold and applies \n    weighted non-maximal suppression.\n        \n    dets：[[xmin,ymin,xmax,ymax,scores],...]\n    thresh：iou 阈值\n    \"\"\"\n    scores = dets[:, 4]\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n\n    areas = (x2 - x1) * (y2 - y1)\n    order = scores.argsort()[::-1]\n    dets =dets[order]\n    max_ids = []\n    weighted_boxes = []\n    while order.size > 0:\n        i = order[0]\n        max_ids.append(i)\n        xx1 = np.maximum(x1[i], x1[order[:]])\n        yy1 = np.maximum(y1[i], y1[order[:]])\n        xx2 = np.minimum(x2[i], x2[order[:]])\n        yy2 = np.minimum(y2[i], y2[order[:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[:]] - inter)\n\n        in_inds = np.where(iou >= thresh)[0]\n        \n        in_dets = dets[in_inds, :]\n\n        weights = in_dets[:, 4] * iou[in_inds]\n        wbox = np.sum((in_dets[:, :4] * weights[..., np.newaxis]), axis=0) \\\n            / np.sum(weights)\n        weighted_boxes.append(wbox)\n\n        out_inds = np.where(iou < thresh)[0]\n        order = order[out_inds]\n        dets = dets[out_inds]\n    scores_frinally=scores[max_ids]\n    return max_ids,scores_frinally, np.array(weighted_boxes)\n```\n### 4、其他思路\n\n以上三种NMS方法都是针对目标检测的效果而设计的算法，在这些NMS算法的实现上首先都用到了排序。在查找资源时，偶然看到一种不需要排序的NMS方式（[浅谈NMS的多种实现](https://zhuanlan.zhihu.com/p/64423753)）。这里我简单介绍一下大佬的实现思路：依次遍历每个框，计算这个框与其他框的iou,找到iou大于一定阈值的其他框，因为这个时候不能保证它一定是score最高的框，所以要进行判断，如果它的score小于其他框，那就把它去掉，因为它肯定不是要保留的框。如果它的score大于其他框，那应该保留它，同时可以去掉所有其他框了。最后保留的框就是结果。其实这种思想就是针对\"标准NMS\"提出的不用排序的另一种解法。\n\npython实现：\n\n```python\ndef nms(bbox, scores, thresh):\n    area=np.prod(bbox[:,2:]-bbox[:,:2],axis=1)\n    keep=np.ones(len(bbox),dtype=bool)\n    for i, b in enumerate(bbox):\n        if(keep[i]==False):\n            continue\n        tl=np.maximum(b[:2],bbox[i+1:,:2])\n        br=np.minimum(b[2:],bbox[i+1:,2:])\n        inter=np.prod(br-tl,axis=1)*(br>=tl).all(axis=1)\n        iou=ia/(area[i+1:]+area[i]-inter)\n        r = [ k for k in np.where(iou>thresh)[0]+i+1 if keep[k]==True]\n        if (scores[i]>scores[r]).all():\n            keep[r]=False\n        else:\n            keep[i]=False\n    return np.where(keep)[0].astype(np.int32)\n```\n\n\n\n相关资源：\n\n[目标检测中的NMS](https://zhuanlan.zhihu.com/p/110256988)\n\n[目标检测中的NMS，soft NMS，softer NMS，Weighted Boxes Fusion](https://blog.csdn.net/practical_sharp/article/details/114980578)\n\n[浅谈NMS的多种实现](https://zhuanlan.zhihu.com/p/64423753)","slug":"目标检测中的NMS","published":1,"updated":"2022-08-29T07:59:42.121Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl7rqq9gy0004jsva7glb5lf2","content":"<p>最近一直在做人脸检测的项目，其中有一个重要过程就是剔除冗余的目标框，该过程利用NMS(Non-Maximum Suppression)方法，最近整理出本人对NMS的理解内容，以及基于Python的实现，如有不对请指出。🍂🍂🍂</p>\n<p>非极大值抑制(Non-Maximum Suppression, NMS),就是抑制不是极大值的元素，多用于目标检测中。一般来说，在目标检测算法输出目标框时，目标框会非常多，其中会有很多重复的框定位到同一个目标上，NMS的作用就是去除冗余框，为每一个目标得到唯一的检测框，<span id=\"more\"></span>如下图所示。</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_1.png\" alt=\"fig.1\"></p>\n<h3 id=\"1、标准的NMS\">1、标准的NMS</h3>\n<p>标准NMS的做法将检测框按得分排序，然后保留得分最高的框，同时删除与该框重叠面积大于一定阈值的其他框。具体的算法流程：</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_5.png\" alt=\"fig.2\"></p>\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">py_cpu_nms</span>(<span class=\"params\">dets,thresh</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    dets：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    thresh：iou 阈值</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    x1=dets[:,<span class=\"number\">0</span>]</span><br><span class=\"line\">    y1=dets[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">    x2=dets[:,<span class=\"number\">2</span>]</span><br><span class=\"line\">    y2=dets[:,<span class=\"number\">3</span>]</span><br><span class=\"line\">    areas=(x2-x1+<span class=\"number\">1</span>)*(y2-y1+<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores=dets[:,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    keep=[]  <span class=\"comment\">#存放nms剩余的方框</span></span><br><span class=\"line\"></span><br><span class=\"line\">    index=scores.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(index)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> index.size&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(index.size)</span><br><span class=\"line\">        i=index[<span class=\"number\">0</span>]</span><br><span class=\"line\">        keep.append(i)</span><br><span class=\"line\">        </span><br><span class=\"line\">        x11 = np.maximum(x1[i], x1[index[<span class=\"number\">1</span>:]])    <span class=\"comment\"># calculate the points of overlap </span></span><br><span class=\"line\">        y11 = np.maximum(y1[i], y1[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        x22 = np.minimum(x2[i], x2[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        y22 = np.minimum(y2[i], y2[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        </span><br><span class=\"line\">        w=np.maximum(<span class=\"number\">0</span>,x22-x11+<span class=\"number\">1</span>)</span><br><span class=\"line\">        h=np.maximum(<span class=\"number\">0</span>,y22-x11+<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        overlaps = w*h</span><br><span class=\"line\"></span><br><span class=\"line\">        ious=overlaps / (areas[i]+areas[index[<span class=\"number\">1</span>:]]-overlaps)        </span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;ious is :&quot;</span>,ious)</span><br><span class=\"line\"></span><br><span class=\"line\">        idx=np.where(ious&lt;thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        index=index[idx+<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(index)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> keep</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"2、soft-NMS：\">2、soft NMS：</h3>\n<p>论文：<a href=\"http://cn.arxiv.org/abs/1704.04503\">Improving Object Detection With One Line of Code</a></p>\n<p>提出soft NMS的作者认为，标准NMS会存在如下图所示的问题，红色框和绿色框是当前的检测结果，二者的得分分别是0.95和0.80。如果按照传统的NMS进行处理，首先选中得分最高的红色框，然后绿色框就会因为与之重叠面积过大而被删掉。另一方面，NMS的阈值也不太容易确定，设小了会出现下图的情况（绿色框因为和红色框重叠面积较大而被删掉），设置过高又容易增大误检。​​</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_3.jpg\" alt></p>\n<p>soft NMS思想：不要删除所有IOU大于阈值的框，而是降低其置信度。具体的算法流程：</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_4.jpg\" alt></p>\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cpu_soft_nms</span>(<span class=\"params\">boxes, sigma=<span class=\"number\">0.5</span>, Nt=<span class=\"number\">0.3</span>, threshold=<span class=\"number\">0.1</span>, method=<span class=\"number\">2</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    boxes：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\">    pos = <span class=\"number\">0</span></span><br><span class=\"line\">    maxscore = <span class=\"number\">0</span></span><br><span class=\"line\">    maxpos = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(N):</span><br><span class=\"line\">        <span class=\"comment\"># 用冒泡排序法找到分数最高的预测框，并将该预测框放在第i个位置</span></span><br><span class=\"line\">        maxscore = boxes[i, <span class=\"number\">4</span>]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 先用一些中间变量存储第i个预测框</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = boxes[i,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; boxes[pos, <span class=\"number\">4</span>]:</span><br><span class=\"line\">                maxscore = boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 将分数最高的预测框M放在第i个位置</span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">4</span>] = boxes[maxpos,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 将原先第i个预测框放在分数最高的位置</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">4</span>] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 程序到此实现了：寻找第i至第N个预测框中分数最高的框，并将其与第i个预测框互换位置。</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 预测框M，前缀&quot;t&quot;表示target</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = boxes[i,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 下面针对M进行NMS迭代过程，</span></span><br><span class=\"line\">        <span class=\"comment\"># 需要注意的是，如果soft-NMS将score削弱至某阈值threshold以下，则将其删除掉</span></span><br><span class=\"line\">        <span class=\"comment\"># 在程序中体现为，将要删除的框放在了最后，并使 N = N-1</span></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (<span class=\"built_in\">min</span>(tx2, x2) - <span class=\"built_in\">max</span>(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (<span class=\"built_in\">min</span>(ty2, y2) - <span class=\"built_in\">max</span>(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    ua = <span class=\"built_in\">float</span>((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    ov = iw * ih / ua  <span class=\"comment\"># iou between max box and detection box</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>:  <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; Nt: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>:  <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:  <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; Nt: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    boxes[pos, <span class=\"number\">4</span>] = weight*boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t    </span><br><span class=\"line\">\t\t            <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">\t\t            <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> boxes[pos, <span class=\"number\">4</span>] &lt; threshold:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">4</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> keep</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、weighted-NMS：\">3、weighted NMS：</h3>\n<p>加权NMS认为传统NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框（得分不是最高的框）也有可能是定位良好的。那么与直接剔除机制不同而是根据网络预测的置信度进行加权，得到新的目标框，把该目标框作为最终的目标框，再将其他框剔除。<br>\n具体的算法流程：</p>\n<!-- ![](./目标检测中的NMS/blog1_fig_4.jpg) -->\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">py_weighted_nms</span>(<span class=\"params\">dets, thresh</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Takes bounding boxes and scores and a threshold and applies </span></span><br><span class=\"line\"><span class=\"string\">    weighted non-maximal suppression.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">    dets：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    thresh：iou 阈值</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    scores = dets[:, <span class=\"number\">4</span>]</span><br><span class=\"line\">    x1 = dets[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">    y1 = dets[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">    x2 = dets[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">    y2 = dets[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    areas = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\">    order = scores.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">    dets =dets[order]</span><br><span class=\"line\">    max_ids = []</span><br><span class=\"line\">    weighted_boxes = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> order.size &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        i = order[<span class=\"number\">0</span>]</span><br><span class=\"line\">        max_ids.append(i)</span><br><span class=\"line\">        xx1 = np.maximum(x1[i], x1[order[:]])</span><br><span class=\"line\">        yy1 = np.maximum(y1[i], y1[order[:]])</span><br><span class=\"line\">        xx2 = np.minimum(x2[i], x2[order[:]])</span><br><span class=\"line\">        yy2 = np.minimum(y2[i], y2[order[:]])</span><br><span class=\"line\"></span><br><span class=\"line\">        w = np.maximum(<span class=\"number\">0.0</span>, xx2 - xx1)</span><br><span class=\"line\">        h = np.maximum(<span class=\"number\">0.0</span>, yy2 - yy1)</span><br><span class=\"line\">        inter = w * h</span><br><span class=\"line\">        iou = inter / (areas[i] + areas[order[:]] - inter)</span><br><span class=\"line\"></span><br><span class=\"line\">        in_inds = np.where(iou &gt;= thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        in_dets = dets[in_inds, :]</span><br><span class=\"line\"></span><br><span class=\"line\">        weights = in_dets[:, <span class=\"number\">4</span>] * iou[in_inds]</span><br><span class=\"line\">        wbox = np.<span class=\"built_in\">sum</span>((in_dets[:, :<span class=\"number\">4</span>] * weights[..., np.newaxis]), axis=<span class=\"number\">0</span>) \\</span><br><span class=\"line\">            / np.<span class=\"built_in\">sum</span>(weights)</span><br><span class=\"line\">        weighted_boxes.append(wbox)</span><br><span class=\"line\"></span><br><span class=\"line\">        out_inds = np.where(iou &lt; thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        order = order[out_inds]</span><br><span class=\"line\">        dets = dets[out_inds]</span><br><span class=\"line\">    scores_frinally=scores[max_ids]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> max_ids,scores_frinally, np.array(weighted_boxes)</span><br></pre></td></tr></table></figure>\n<h3 id=\"4、其他思路\">4、其他思路</h3>\n<p>以上三种NMS方法都是针对目标检测的效果而设计的算法，在这些NMS算法的实现上首先都用到了排序。在查找资源时，偶然看到一种不需要排序的NMS方式（<a href=\"https://zhuanlan.zhihu.com/p/64423753\">浅谈NMS的多种实现</a>）。这里我简单介绍一下大佬的实现思路：依次遍历每个框，计算这个框与其他框的iou,找到iou大于一定阈值的其他框，因为这个时候不能保证它一定是score最高的框，所以要进行判断，如果它的score小于其他框，那就把它去掉，因为它肯定不是要保留的框。如果它的score大于其他框，那应该保留它，同时可以去掉所有其他框了。最后保留的框就是结果。其实这种思想就是针对&quot;标准NMS&quot;提出的不用排序的另一种解法。</p>\n<p>python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nms</span>(<span class=\"params\">bbox, scores, thresh</span>):</span></span><br><span class=\"line\">    area=np.prod(bbox[:,<span class=\"number\">2</span>:]-bbox[:,:<span class=\"number\">2</span>],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    keep=np.ones(<span class=\"built_in\">len</span>(bbox),dtype=<span class=\"built_in\">bool</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, b <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(bbox):</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(keep[i]==<span class=\"literal\">False</span>):</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        tl=np.maximum(b[:<span class=\"number\">2</span>],bbox[i+<span class=\"number\">1</span>:,:<span class=\"number\">2</span>])</span><br><span class=\"line\">        br=np.minimum(b[<span class=\"number\">2</span>:],bbox[i+<span class=\"number\">1</span>:,<span class=\"number\">2</span>:])</span><br><span class=\"line\">        inter=np.prod(br-tl,axis=<span class=\"number\">1</span>)*(br&gt;=tl).<span class=\"built_in\">all</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        iou=ia/(area[i+<span class=\"number\">1</span>:]+area[i]-inter)</span><br><span class=\"line\">        r = [ k <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> np.where(iou&gt;thresh)[<span class=\"number\">0</span>]+i+<span class=\"number\">1</span> <span class=\"keyword\">if</span> keep[k]==<span class=\"literal\">True</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (scores[i]&gt;scores[r]).<span class=\"built_in\">all</span>():</span><br><span class=\"line\">            keep[r]=<span class=\"literal\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            keep[i]=<span class=\"literal\">False</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.where(keep)[<span class=\"number\">0</span>].astype(np.int32)</span><br></pre></td></tr></table></figure>\n<p>相关资源：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/110256988\">目标检测中的NMS</a></p>\n<p><a href=\"https://blog.csdn.net/practical_sharp/article/details/114980578\">目标检测中的NMS，soft NMS，softer NMS，Weighted Boxes Fusion</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64423753\">浅谈NMS的多种实现</a></p>\n","site":{"data":{}},"excerpt":"<p>最近一直在做人脸检测的项目，其中有一个重要过程就是剔除冗余的目标框，该过程利用NMS(Non-Maximum Suppression)方法，最近整理出本人对NMS的理解内容，以及基于Python的实现，如有不对请指出。🍂🍂🍂</p>\n<p>非极大值抑制(Non-Maximum Suppression, NMS),就是抑制不是极大值的元素，多用于目标检测中。一般来说，在目标检测算法输出目标框时，目标框会非常多，其中会有很多重复的框定位到同一个目标上，NMS的作用就是去除冗余框，为每一个目标得到唯一的检测框，</p>","more":"如下图所示。<p></p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_1.png\" alt=\"fig.1\"></p>\n<h3 id=\"1、标准的NMS\">1、标准的NMS</h3>\n<p>标准NMS的做法将检测框按得分排序，然后保留得分最高的框，同时删除与该框重叠面积大于一定阈值的其他框。具体的算法流程：</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_5.png\" alt=\"fig.2\"></p>\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">py_cpu_nms</span>(<span class=\"params\">dets,thresh</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    dets：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    thresh：iou 阈值</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    x1=dets[:,<span class=\"number\">0</span>]</span><br><span class=\"line\">    y1=dets[:,<span class=\"number\">1</span>]</span><br><span class=\"line\">    x2=dets[:,<span class=\"number\">2</span>]</span><br><span class=\"line\">    y2=dets[:,<span class=\"number\">3</span>]</span><br><span class=\"line\">    areas=(x2-x1+<span class=\"number\">1</span>)*(y2-y1+<span class=\"number\">1</span>)</span><br><span class=\"line\">    scores=dets[:,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    keep=[]  <span class=\"comment\">#存放nms剩余的方框</span></span><br><span class=\"line\"></span><br><span class=\"line\">    index=scores.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(index)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> index.size&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(index.size)</span><br><span class=\"line\">        i=index[<span class=\"number\">0</span>]</span><br><span class=\"line\">        keep.append(i)</span><br><span class=\"line\">        </span><br><span class=\"line\">        x11 = np.maximum(x1[i], x1[index[<span class=\"number\">1</span>:]])    <span class=\"comment\"># calculate the points of overlap </span></span><br><span class=\"line\">        y11 = np.maximum(y1[i], y1[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        x22 = np.minimum(x2[i], x2[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        y22 = np.minimum(y2[i], y2[index[<span class=\"number\">1</span>:]])</span><br><span class=\"line\">        </span><br><span class=\"line\">        w=np.maximum(<span class=\"number\">0</span>,x22-x11+<span class=\"number\">1</span>)</span><br><span class=\"line\">        h=np.maximum(<span class=\"number\">0</span>,y22-x11+<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        overlaps = w*h</span><br><span class=\"line\"></span><br><span class=\"line\">        ious=overlaps / (areas[i]+areas[index[<span class=\"number\">1</span>:]]-overlaps)        </span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;ious is :&quot;</span>,ious)</span><br><span class=\"line\"></span><br><span class=\"line\">        idx=np.where(ious&lt;thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        index=index[idx+<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(index)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> keep</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"2、soft-NMS：\">2、soft NMS：</h3>\n<p>论文：<a href=\"http://cn.arxiv.org/abs/1704.04503\">Improving Object Detection With One Line of Code</a></p>\n<p>提出soft NMS的作者认为，标准NMS会存在如下图所示的问题，红色框和绿色框是当前的检测结果，二者的得分分别是0.95和0.80。如果按照传统的NMS进行处理，首先选中得分最高的红色框，然后绿色框就会因为与之重叠面积过大而被删掉。另一方面，NMS的阈值也不太容易确定，设小了会出现下图的情况（绿色框因为和红色框重叠面积较大而被删掉），设置过高又容易增大误检。​​</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_3.jpg\" alt></p>\n<p>soft NMS思想：不要删除所有IOU大于阈值的框，而是降低其置信度。具体的算法流程：</p>\n<p><img src=\"/2021/06/13/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84NMS/blog1_fig_4.jpg\" alt></p>\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cpu_soft_nms</span>(<span class=\"params\">boxes, sigma=<span class=\"number\">0.5</span>, Nt=<span class=\"number\">0.3</span>, threshold=<span class=\"number\">0.1</span>, method=<span class=\"number\">2</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    boxes：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    N = boxes.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"> </span><br><span class=\"line\">    pos = <span class=\"number\">0</span></span><br><span class=\"line\">    maxscore = <span class=\"number\">0</span></span><br><span class=\"line\">    maxpos = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(N):</span><br><span class=\"line\">        <span class=\"comment\"># 用冒泡排序法找到分数最高的预测框，并将该预测框放在第i个位置</span></span><br><span class=\"line\">        maxscore = boxes[i, <span class=\"number\">4</span>]</span><br><span class=\"line\">        maxpos = i</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 先用一些中间变量存储第i个预测框</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = boxes[i,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># get max box</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> maxscore &lt; boxes[pos, <span class=\"number\">4</span>]:</span><br><span class=\"line\">                maxscore = boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\">                maxpos = pos</span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 将分数最高的预测框M放在第i个位置</span></span><br><span class=\"line\">        boxes[i,<span class=\"number\">0</span>] = boxes[maxpos,<span class=\"number\">0</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">1</span>] = boxes[maxpos,<span class=\"number\">1</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">2</span>] = boxes[maxpos,<span class=\"number\">2</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">3</span>] = boxes[maxpos,<span class=\"number\">3</span>]</span><br><span class=\"line\">        boxes[i,<span class=\"number\">4</span>] = boxes[maxpos,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 将原先第i个预测框放在分数最高的位置</span></span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">0</span>] = tx1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">1</span>] = ty1</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">2</span>] = tx2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">3</span>] = ty2</span><br><span class=\"line\">        boxes[maxpos,<span class=\"number\">4</span>] = ts</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 程序到此实现了：寻找第i至第N个预测框中分数最高的框，并将其与第i个预测框互换位置。</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 预测框M，前缀&quot;t&quot;表示target</span></span><br><span class=\"line\">        tx1 = boxes[i,<span class=\"number\">0</span>]</span><br><span class=\"line\">        ty1 = boxes[i,<span class=\"number\">1</span>]</span><br><span class=\"line\">        tx2 = boxes[i,<span class=\"number\">2</span>]</span><br><span class=\"line\">        ty2 = boxes[i,<span class=\"number\">3</span>]</span><br><span class=\"line\">        ts = boxes[i,<span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">\t    <span class=\"comment\"># 下面针对M进行NMS迭代过程，</span></span><br><span class=\"line\">        <span class=\"comment\"># 需要注意的是，如果soft-NMS将score削弱至某阈值threshold以下，则将其删除掉</span></span><br><span class=\"line\">        <span class=\"comment\"># 在程序中体现为，将要删除的框放在了最后，并使 N = N-1</span></span><br><span class=\"line\">        pos = i + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> pos &lt; N:</span><br><span class=\"line\">            x1 = boxes[pos, <span class=\"number\">0</span>]</span><br><span class=\"line\">            y1 = boxes[pos, <span class=\"number\">1</span>]</span><br><span class=\"line\">            x2 = boxes[pos, <span class=\"number\">2</span>]</span><br><span class=\"line\">            y2 = boxes[pos, <span class=\"number\">3</span>]</span><br><span class=\"line\">            s = boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">            area = (x2 - x1 + <span class=\"number\">1</span>) * (y2 - y1 + <span class=\"number\">1</span>)</span><br><span class=\"line\">            iw = (<span class=\"built_in\">min</span>(tx2, x2) - <span class=\"built_in\">max</span>(tx1, x1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> iw &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                ih = (<span class=\"built_in\">min</span>(ty2, y2) - <span class=\"built_in\">max</span>(ty1, y1) + <span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"keyword\">if</span> ih &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">                    ua = <span class=\"built_in\">float</span>((tx2 - tx1 + <span class=\"number\">1</span>) * (ty2 - ty1 + <span class=\"number\">1</span>) + area - iw * ih)</span><br><span class=\"line\">                    ov = iw * ih / ua  <span class=\"comment\"># iou between max box and detection box</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> method == <span class=\"number\">1</span>:  <span class=\"comment\"># linear</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; Nt: </span><br><span class=\"line\">                            weight = <span class=\"number\">1</span> - ov</span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\">                    <span class=\"keyword\">elif</span> method == <span class=\"number\">2</span>:  <span class=\"comment\"># gaussian</span></span><br><span class=\"line\">                        weight = np.exp(-(ov * ov)/sigma)</span><br><span class=\"line\">                    <span class=\"keyword\">else</span>:  <span class=\"comment\"># original NMS</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> ov &gt; Nt: </span><br><span class=\"line\">                            weight = <span class=\"number\">0</span></span><br><span class=\"line\">                        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                            weight = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">                    boxes[pos, <span class=\"number\">4</span>] = weight*boxes[pos, <span class=\"number\">4</span>]</span><br><span class=\"line\">\t\t    </span><br><span class=\"line\">\t\t            <span class=\"comment\"># if box score falls below threshold, discard the box by swapping with last box</span></span><br><span class=\"line\">\t\t            <span class=\"comment\"># update N</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> boxes[pos, <span class=\"number\">4</span>] &lt; threshold:</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">0</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">0</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">1</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">2</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">2</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">3</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\">                        boxes[pos,<span class=\"number\">4</span>] = boxes[N-<span class=\"number\">1</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">                        N = N - <span class=\"number\">1</span></span><br><span class=\"line\">                        pos = pos - <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">            pos = pos + <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    keep = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(N)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> keep</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、weighted-NMS：\">3、weighted NMS：</h3>\n<p>加权NMS认为传统NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框（得分不是最高的框）也有可能是定位良好的。那么与直接剔除机制不同而是根据网络预测的置信度进行加权，得到新的目标框，把该目标框作为最终的目标框，再将其他框剔除。<br>\n具体的算法流程：</p>\n<!-- ![](./目标检测中的NMS/blog1_fig_4.jpg) -->\n<p>Python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">py_weighted_nms</span>(<span class=\"params\">dets, thresh</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    Takes bounding boxes and scores and a threshold and applies </span></span><br><span class=\"line\"><span class=\"string\">    weighted non-maximal suppression.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">    dets：[[xmin,ymin,xmax,ymax,scores],...]</span></span><br><span class=\"line\"><span class=\"string\">    thresh：iou 阈值</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    scores = dets[:, <span class=\"number\">4</span>]</span><br><span class=\"line\">    x1 = dets[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">    y1 = dets[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">    x2 = dets[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">    y2 = dets[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    areas = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\">    order = scores.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">    dets =dets[order]</span><br><span class=\"line\">    max_ids = []</span><br><span class=\"line\">    weighted_boxes = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> order.size &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        i = order[<span class=\"number\">0</span>]</span><br><span class=\"line\">        max_ids.append(i)</span><br><span class=\"line\">        xx1 = np.maximum(x1[i], x1[order[:]])</span><br><span class=\"line\">        yy1 = np.maximum(y1[i], y1[order[:]])</span><br><span class=\"line\">        xx2 = np.minimum(x2[i], x2[order[:]])</span><br><span class=\"line\">        yy2 = np.minimum(y2[i], y2[order[:]])</span><br><span class=\"line\"></span><br><span class=\"line\">        w = np.maximum(<span class=\"number\">0.0</span>, xx2 - xx1)</span><br><span class=\"line\">        h = np.maximum(<span class=\"number\">0.0</span>, yy2 - yy1)</span><br><span class=\"line\">        inter = w * h</span><br><span class=\"line\">        iou = inter / (areas[i] + areas[order[:]] - inter)</span><br><span class=\"line\"></span><br><span class=\"line\">        in_inds = np.where(iou &gt;= thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\">        in_dets = dets[in_inds, :]</span><br><span class=\"line\"></span><br><span class=\"line\">        weights = in_dets[:, <span class=\"number\">4</span>] * iou[in_inds]</span><br><span class=\"line\">        wbox = np.<span class=\"built_in\">sum</span>((in_dets[:, :<span class=\"number\">4</span>] * weights[..., np.newaxis]), axis=<span class=\"number\">0</span>) \\</span><br><span class=\"line\">            / np.<span class=\"built_in\">sum</span>(weights)</span><br><span class=\"line\">        weighted_boxes.append(wbox)</span><br><span class=\"line\"></span><br><span class=\"line\">        out_inds = np.where(iou &lt; thresh)[<span class=\"number\">0</span>]</span><br><span class=\"line\">        order = order[out_inds]</span><br><span class=\"line\">        dets = dets[out_inds]</span><br><span class=\"line\">    scores_frinally=scores[max_ids]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> max_ids,scores_frinally, np.array(weighted_boxes)</span><br></pre></td></tr></table></figure>\n<h3 id=\"4、其他思路\">4、其他思路</h3>\n<p>以上三种NMS方法都是针对目标检测的效果而设计的算法，在这些NMS算法的实现上首先都用到了排序。在查找资源时，偶然看到一种不需要排序的NMS方式（<a href=\"https://zhuanlan.zhihu.com/p/64423753\">浅谈NMS的多种实现</a>）。这里我简单介绍一下大佬的实现思路：依次遍历每个框，计算这个框与其他框的iou,找到iou大于一定阈值的其他框，因为这个时候不能保证它一定是score最高的框，所以要进行判断，如果它的score小于其他框，那就把它去掉，因为它肯定不是要保留的框。如果它的score大于其他框，那应该保留它，同时可以去掉所有其他框了。最后保留的框就是结果。其实这种思想就是针对&quot;标准NMS&quot;提出的不用排序的另一种解法。</p>\n<p>python实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nms</span>(<span class=\"params\">bbox, scores, thresh</span>):</span></span><br><span class=\"line\">    area=np.prod(bbox[:,<span class=\"number\">2</span>:]-bbox[:,:<span class=\"number\">2</span>],axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    keep=np.ones(<span class=\"built_in\">len</span>(bbox),dtype=<span class=\"built_in\">bool</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, b <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(bbox):</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(keep[i]==<span class=\"literal\">False</span>):</span><br><span class=\"line\">            <span class=\"keyword\">continue</span></span><br><span class=\"line\">        tl=np.maximum(b[:<span class=\"number\">2</span>],bbox[i+<span class=\"number\">1</span>:,:<span class=\"number\">2</span>])</span><br><span class=\"line\">        br=np.minimum(b[<span class=\"number\">2</span>:],bbox[i+<span class=\"number\">1</span>:,<span class=\"number\">2</span>:])</span><br><span class=\"line\">        inter=np.prod(br-tl,axis=<span class=\"number\">1</span>)*(br&gt;=tl).<span class=\"built_in\">all</span>(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">        iou=ia/(area[i+<span class=\"number\">1</span>:]+area[i]-inter)</span><br><span class=\"line\">        r = [ k <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> np.where(iou&gt;thresh)[<span class=\"number\">0</span>]+i+<span class=\"number\">1</span> <span class=\"keyword\">if</span> keep[k]==<span class=\"literal\">True</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (scores[i]&gt;scores[r]).<span class=\"built_in\">all</span>():</span><br><span class=\"line\">            keep[r]=<span class=\"literal\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            keep[i]=<span class=\"literal\">False</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> np.where(keep)[<span class=\"number\">0</span>].astype(np.int32)</span><br></pre></td></tr></table></figure>\n<p>相关资源：</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/110256988\">目标检测中的NMS</a></p>\n<p><a href=\"https://blog.csdn.net/practical_sharp/article/details/114980578\">目标检测中的NMS，soft NMS，softer NMS，Weighted Boxes Fusion</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64423753\">浅谈NMS的多种实现</a></p>"}],"PostAsset":[{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905153749362.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905153749362.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905155032987.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905155032987.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905162418404.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905162418404.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905184019018.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905184019018.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905192310760.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905192310760.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905192726753.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905192726753.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905194211096.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905194211096.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195112074.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905195112074.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195532167.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905195532167.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905195923509.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905195923509.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905202018226.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905202018226.png","modified":1,"renderable":1},{"_id":"source/_posts/HLA-Face阅读笔记/image-20210905203317458.png","post":"cl7rqq9go0001jsvaamjq8wuo","slug":"image-20210905203317458.png","modified":1,"renderable":1},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_1.png","post":"cl7rqq9gy0004jsva7glb5lf2","slug":"blog1_fig_1.png","modified":1,"renderable":1},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_2.png","post":"cl7rqq9gy0004jsva7glb5lf2","slug":"blog1_fig_2.png","modified":1,"renderable":1},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_3.jpg","post":"cl7rqq9gy0004jsva7glb5lf2","slug":"blog1_fig_3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_4.jpg","post":"cl7rqq9gy0004jsva7glb5lf2","slug":"blog1_fig_4.jpg","modified":1,"renderable":1},{"_id":"source/_posts/目标检测中的NMS/blog1_fig_5.png","post":"cl7rqq9gy0004jsva7glb5lf2","slug":"blog1_fig_5.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cl7rqq9gu0002jsva31b1833i","category_id":"cl7rqq9h30006jsva8u5k0dhr","_id":"cl7rqq9hb000ajsvadddu292u"},{"post_id":"cl7rqq9go0001jsvaamjq8wuo","category_id":"cl7rqq9gw0003jsvaa0km1muh","_id":"cl7rqq9hd000cjsva1b333527"},{"post_id":"cl7rqq9go0001jsvaamjq8wuo","category_id":"cl7rqq9hb0009jsvadh8z3rg0","_id":"cl7rqq9hd000djsva1zx16ia7"},{"post_id":"cl7rqq9gy0004jsva7glb5lf2","category_id":"cl7rqq9gw0003jsvaa0km1muh","_id":"cl7rqq9hd000ejsva0k1kccac"},{"post_id":"cl7rqq9gy0004jsva7glb5lf2","category_id":"cl7rqq9hb0009jsvadh8z3rg0","_id":"cl7rqq9hd000fjsva50ihf57u"}],"PostTag":[{"post_id":"cl7rqq9gy0004jsva7glb5lf2","tag_id":"cl7rqq9h10005jsvah0ejewuf","_id":"cl7rqq9ha0008jsva012le2ec"}],"Tag":[{"name":"随笔","_id":"cl7rqq9h10005jsvah0ejewuf"}]}}